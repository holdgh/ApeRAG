# LightRAG å…¨å±€çŠ¶æ€é—®é¢˜ç²¾å‡†è§£å†³æ–¹æ¡ˆ

## é—®é¢˜é‡æ–°åˆ†æ

ç»è¿‡æ·±å…¥ä»£ç åˆ†æï¼Œå‘ç°LightRAGçš„é”åˆ†ä¸ºä¸¤ç±»ï¼š

### ğŸ”’ **å­˜å‚¨ä¿æŠ¤é”ï¼ˆå¿…é¡»ä¿ç•™ï¼‰**
è¿™äº›é”ä¿æŠ¤çš„æ˜¯å®é™…çš„æ•°æ®ä¸€è‡´æ€§ï¼Œåˆ é™¤ä¼šå¯¼è‡´æ•°æ®æŸåï¼š

1. **`_storage_lock`** - ä¿æŠ¤æ–‡ä»¶å‹å­˜å‚¨çš„è¯»å†™æ“ä½œ
2. **`_graph_db_lock`** - ä¿æŠ¤å›¾æ•°æ®åº“æ“ä½œçš„åŸå­æ€§

### ğŸ—‘ï¸ **ç®¡é“çŠ¶æ€é”ï¼ˆå¯ä»¥åˆ é™¤ï¼‰**
è¿™äº›é”åªç”¨äºç®¡é“çŠ¶æ€ç®¡ç†ï¼Œåˆ é™¤ä¸ä¼šå½±å“æ•°æ®ä¸€è‡´æ€§ï¼š

1. **`_pipeline_status_lock`** - æ§åˆ¶`pipeline_status["busy"]`å…¨å±€äº’æ–¥
2. **`_internal_lock`** - ç”¨äºshared_storageå†…éƒ¨çŠ¶æ€ç®¡ç†

## ä¸ºä»€ä¹ˆå­˜å‚¨é”ä¸èƒ½åˆ é™¤ï¼Ÿ

### æ–‡ä»¶å­˜å‚¨çš„å¹¶å‘é—®é¢˜

ä»¥`JsonKVStorage`ä¸ºä¾‹ï¼Œå®ƒçš„å·¥ä½œæ¨¡å¼æ˜¯ï¼š

```python
# æ•°æ®æµï¼šå†…å­˜ â†” æ–‡ä»¶
class JsonKVStorage:
    def __init__(self):
        self._data = {}  # å†…å­˜ä¸­çš„æ•°æ®å­—å…¸
        self._file_name = "xxx.json"  # å¯¹åº”çš„JSONæ–‡ä»¶
        
    async def upsert(self, data):
        async with self._storage_lock:  # å¿…éœ€ï¼ä¿æŠ¤å†…å­˜æ•°æ®
            self._data.update(data)  # ä¿®æ”¹å†…å­˜
            await set_all_update_flags()  # æ ‡è®°éœ€è¦æŒä¹…åŒ–
            
    async def index_done_callback(self):
        async with self._storage_lock:  # å¿…éœ€ï¼ä¿æŠ¤æ–‡ä»¶å†™å…¥
            write_json(self._data, self._file_name)  # æŒä¹…åŒ–åˆ°æ–‡ä»¶
```

**å¦‚æœåˆ é™¤`_storage_lock`ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**

1. **å†…å­˜æ•°æ®ç«äº‰**ï¼šå¤šä¸ªåç¨‹åŒæ—¶ä¿®æ”¹`self._data`å­—å…¸ï¼Œå¯¼è‡´æ•°æ®ä¸ä¸€è‡´
2. **æ–‡ä»¶å†™å…¥ç«äº‰**ï¼šå¤šä¸ªè¿›ç¨‹åŒæ—¶å†™å…¥åŒä¸€JSONæ–‡ä»¶ï¼Œå¯¼è‡´æ–‡ä»¶æŸå
3. **è¯»å†™å†²çª**ï¼šä¸€ä¸ªè¿›ç¨‹åœ¨å†™æ–‡ä»¶æ—¶ï¼Œå¦ä¸€ä¸ªè¿›ç¨‹åŒæ—¶è¯»å–ï¼Œå¾—åˆ°ä¸å®Œæ•´æ•°æ®

### å‘é‡æ•°æ®åº“çš„å¹¶å‘é—®é¢˜

`NanoVectorDBStorage`ä¹Ÿæœ‰ç±»ä¼¼é—®é¢˜ï¼š

```python
async def upsert(self, data):
    client = await self._get_client()  # éœ€è¦é”ä¿æŠ¤
    results = client.upsert(datas=list_data)  # éœ€è¦é”ä¿æŠ¤

async def _get_client(self):
    async with self._storage_lock:  # å¿…éœ€ï¼æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
        if self.storage_updated.value:
            self._client = NanoVectorDB(...)  # é‡å»ºå®¢æˆ·ç«¯
```

## ç²¾ç¡®çš„è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆï¼šä¿ç•™å­˜å‚¨é”ï¼Œç§»é™¤ç®¡é“é”

```python
# aperag/graph/lightrag/kg/shared_storage.py

# ğŸ”’ ä¿ç•™è¿™äº›é”ï¼ˆæ•°æ®ä¿æŠ¤ï¼‰
_storage_lock: Optional[LockType] = None      # âœ… ä¿ç•™ï¼šä¿æŠ¤æ–‡ä»¶å­˜å‚¨
_graph_db_lock: Optional[LockType] = None     # âœ… ä¿ç•™ï¼šä¿æŠ¤å›¾æ•°æ®åº“

# ğŸ—‘ï¸ åˆ é™¤è¿™äº›é”ï¼ˆç®¡é“çŠ¶æ€ï¼‰
# _pipeline_status_lock: Optional[LockType] = None  # âŒ åˆ é™¤ï¼šç®¡é“çŠ¶æ€é”
# _internal_lock: Optional[LockType] = None          # âŒ åˆ é™¤ï¼šå†…éƒ¨çŠ¶æ€é”

# ğŸ—‘ï¸ åˆ é™¤è¿™äº›å…¨å±€çŠ¶æ€ï¼ˆç®¡é“ç›¸å…³ï¼‰
# _shared_dicts: Optional[Dict[str, Any]] = None     # âŒ åˆ é™¤ï¼šå…¨å±€çŠ¶æ€å­—å…¸
# _init_flags: Optional[Dict[str, bool]] = None      # âŒ åˆ é™¤ï¼šåˆå§‹åŒ–æ ‡è®°
# _update_flags: Optional[Dict[str, bool]] = None    # âŒ åˆ é™¤ï¼šæ›´æ–°æ ‡è®°
```

### ä¿®æ”¹åçš„æ— çŠ¶æ€æ¥å£

```python
# aperag/graph/lightrag/lightrag.py

async def aprocess_graph_indexing(
    self,
    chunks: dict[str, Any],
    collection_id: str | None = None,
) -> dict[str, Any]:
    """æ— çŠ¶æ€å›¾ç´¢å¼•æ„å»º"""
    try:
        # 1. å®ä½“å…³ç³»æŠ½å– - ä¸ä¼ ç®¡é“çŠ¶æ€
        chunk_results = await extract_entities(
            chunks,
            global_config=asdict(self),
            pipeline_status=None,      # âŒ ä¸ä¼ é€’ç®¡é“çŠ¶æ€
            pipeline_status_lock=None, # âŒ ä¸ä¼ é€’ç®¡é“é”
            llm_response_cache=self.llm_response_cache,
        )
        
        # 2. åˆå¹¶èŠ‚ç‚¹å’Œè¾¹ - ä»ç„¶ä½¿ç”¨å­˜å‚¨é”ä¿æŠ¤æ•°æ®
        await merge_nodes_and_edges(
            chunk_results=chunk_results,
            knowledge_graph_inst=self.chunk_entity_relation_graph,
            entity_vdb=self.entities_vdb,
            relationships_vdb=self.relationships_vdb,
            global_config=asdict(self),
            pipeline_status=None,      # âŒ ä¸ä¼ é€’ç®¡é“çŠ¶æ€
            pipeline_status_lock=None, # âŒ ä¸ä¼ é€’ç®¡é“é”
            llm_response_cache=self.llm_response_cache,
            # âœ… graph_db_lockåœ¨merge_nodes_and_edgeså†…éƒ¨ä»ç„¶ä½¿ç”¨
        )
        
        return {"status": "success", ...}
    except Exception as e:
        return {"status": "error", "error": str(e)}
```

### ä¿®æ”¹operate.pyå¤„ç†Noneé”

```python
# aperag/graph/lightrag/operate.py

async def _merge_nodes_then_upsert(
    entity_name: str,
    nodes_data: list[dict],
    knowledge_graph_inst: BaseGraphStorage,
    global_config: dict,
    pipeline_status: dict = None,      # ç°åœ¨å¯èƒ½ä¸ºNone
    pipeline_status_lock=None,         # ç°åœ¨å¯èƒ½ä¸ºNone
    llm_response_cache: BaseKVStorage | None = None,
):
    # ... åˆå¹¶é€»è¾‘ ...
    
    # ğŸ”§ å®‰å…¨åœ°å¤„ç†ç®¡é“çŠ¶æ€æ›´æ–°
    if pipeline_status is not None and pipeline_status_lock is not None:
        async with pipeline_status_lock:
            pipeline_status["latest_message"] = status_message
            pipeline_status["history_messages"].append(status_message)
    else:
        # æ— ç®¡é“çŠ¶æ€æ—¶ï¼Œä»ç„¶è®°å½•æ—¥å¿—
        logger.info(status_message)
    
    # ç»§ç»­å¤„ç†...
```

### ä¿®æ”¹merge_nodes_and_edges

```python
async def merge_nodes_and_edges(
    chunk_results: list,
    knowledge_graph_inst: BaseGraphStorage,
    entity_vdb: BaseVectorStorage,
    relationships_vdb: BaseVectorStorage,
    global_config: dict[str, str],
    pipeline_status: dict = None,        # å¯èƒ½ä¸ºNone
    pipeline_status_lock=None,           # å¯èƒ½ä¸ºNone
    llm_response_cache: BaseKVStorage | None = None,
    current_file_number: int = 0,
    total_files: int = 0,
    file_path: str = "unknown_source",
) -> None:
    # è·å–å›¾æ•°æ®åº“é” - è¿™ä¸ªå¿…é¡»ä¿ç•™ï¼
    from .kg.shared_storage import get_graph_db_lock
    graph_db_lock = get_graph_db_lock(enable_logging=False)
    
    # ... æ”¶é›†èŠ‚ç‚¹å’Œè¾¹ ...
    
    async with graph_db_lock:  # âœ… ä¿ç•™ï¼šä¿æŠ¤å›¾æ•°æ®åº“æ“ä½œ
        # ğŸ”§ å®‰å…¨åœ°å¤„ç†ç®¡é“çŠ¶æ€æ›´æ–°
        if pipeline_status_lock is not None:
            async with pipeline_status_lock:
                log_message = f"Merging stage {current_file_number}/{total_files}: {file_path}"
                logger.info(log_message)
                if pipeline_status is not None:
                    pipeline_status["latest_message"] = log_message
                    pipeline_status["history_messages"].append(log_message)
        else:
            # æ— ç®¡é“çŠ¶æ€æ—¶ï¼Œä»ç„¶è®°å½•æ—¥å¿—
            log_message = f"Merging stage {current_file_number}/{total_files}: {file_path}"
            logger.info(log_message)
        
        # ... å¤„ç†å®ä½“å’Œå…³ç³» ...
```

## å…·ä½“å®æ–½æ­¥éª¤

### æ­¥éª¤1ï¼šä¿®æ”¹shared_storage.py
```python
def initialize_share_data(workers: int = 1):
    """ç®€åŒ–ç‰ˆçš„åˆå§‹åŒ–ï¼Œåªä¿ç•™å­˜å‚¨ç›¸å…³çš„é”"""
    global _storage_lock, _graph_db_lock, _initialized
    
    if _initialized:
        return
    
    if workers > 1:
        _manager = Manager()
        _storage_lock = _manager.Lock()
        _graph_db_lock = _manager.Lock()
    else:
        _storage_lock = asyncio.Lock()
        _graph_db_lock = asyncio.Lock()
    
    _initialized = True

# ç§»é™¤è¿™äº›å‡½æ•°
# def get_pipeline_status_lock():  # âŒ åˆ é™¤
# def get_internal_lock():         # âŒ åˆ é™¤
# def get_namespace_data():        # âŒ åˆ é™¤
```

### æ­¥éª¤2ï¼šä¿®æ”¹LightRAG.__post_init__
```python
def __post_init__(self):
    # åªè°ƒç”¨ç®€åŒ–çš„åˆå§‹åŒ–
    initialize_share_data()  # åªåˆå§‹åŒ–å­˜å‚¨é”
    
    # ç§»é™¤ç®¡é“çŠ¶æ€ç›¸å…³åˆå§‹åŒ–
    # ä¸å†è°ƒç”¨get_namespace_dataç­‰å‡½æ•°
```

### æ­¥éª¤3ï¼šæ¸…ç†å­˜å‚¨å®ç°
```python
# åœ¨å„ä¸ªå­˜å‚¨å®ç°ä¸­ç§»é™¤å¯¹ç®¡é“çŠ¶æ€çš„ä¾èµ–
class JsonKVStorage(BaseKVStorage):
    async def initialize(self):
        self._storage_lock = get_storage_lock()  # âœ… ä¿ç•™å­˜å‚¨é”
        # ç§»é™¤update_flagç›¸å…³é€»è¾‘
        
    async def upsert(self, data):
        async with self._storage_lock:  # âœ… ä¿ç•™ï¼šä¿æŠ¤æ•°æ®
            self._data.update(data)
            # ç§»é™¤set_all_update_flagsè°ƒç”¨
```

## æ”¹é€ åçš„ä¼˜åŠ¿

1. **çœŸæ­£çš„å¹¶å‘**ï¼šç§»é™¤ç®¡é“çŠ¶æ€é”åï¼Œå¤šä¸ªcollectionå¯ä»¥å¹¶å‘å¤„ç†
2. **æ•°æ®å®‰å…¨**ï¼šä¿ç•™å­˜å‚¨é”ï¼Œç¡®ä¿æ–‡ä»¶å’Œæ•°æ®åº“æ“ä½œçš„å®‰å…¨
3. **ç®€åŒ–æ¶æ„**ï¼šç§»é™¤å¤æ‚çš„å…¨å±€çŠ¶æ€ç®¡ç†
4. **å‘åå…¼å®¹**ï¼šå­˜å‚¨æ¥å£ä¿æŒä¸å˜

## é£é™©æ§åˆ¶

1. **å­˜å‚¨é”å¿…é¡»ä¿ç•™**ï¼šå¦åˆ™ä¼šå¯¼è‡´æ•°æ®æŸå
2. **å›¾é”å»ºè®®ä¿ç•™**ï¼šç¡®ä¿å®ä½“å…³ç³»åˆå¹¶çš„åŸå­æ€§
3. **æ¸è¿›å¼ä¿®æ”¹**ï¼šå…ˆä¿®æ”¹operate.pyæ”¯æŒNoneå‚æ•°ï¼Œå†åˆ é™¤å…¨å±€çŠ¶æ€

## æµ‹è¯•éªŒè¯

```python
async def test_concurrent_processing():
    """æµ‹è¯•å¹¶å‘å¤„ç†"""
    
    # åˆ›å»ºå¤šä¸ªLightRAGå®ä¾‹
    rags = [
        LightRAG(working_dir=f"./test_rag_{i}", workspace=f"collection_{i}")
        for i in range(3)
    ]
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_chunks = [
        {"chunk-1": {"content": f"Test document {i}", "full_doc_id": f"doc-{i}"}}
        for i in range(3)
    ]
    
    # å¹¶å‘æµ‹è¯•å›¾ç´¢å¼•æ„å»º
    tasks = [
        rag.aprocess_graph_indexing(chunks, f"collection_{i}")
        for i, (rag, chunks) in enumerate(zip(rags, test_chunks))
    ]
    
    # è¿™åº”è¯¥èƒ½å¤ŸçœŸæ­£å¹¶å‘æ‰§è¡Œï¼Œè€Œä¸ä¼šè¢«pipeline_status_locké˜»å¡
    results = await asyncio.gather(*tasks)
    
    # éªŒè¯æ‰€æœ‰ä»»åŠ¡éƒ½æˆåŠŸ
    assert all(r["status"] == "success" for r in results)
```

è¿™ä¸ªæ–¹æ¡ˆæ—¢ä¿è¯äº†æ•°æ®å®‰å…¨ï¼Œåˆå®ç°äº†çœŸæ­£çš„å¹¶å‘èƒ½åŠ›ï¼ 