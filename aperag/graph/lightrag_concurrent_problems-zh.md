# LightRAG å¹¶å‘æ§åˆ¶æœºåˆ¶æ·±åº¦è§£æ

## æ¦‚è¿°

LightRAG å­˜åœ¨å¤šå±‚æ¬¡çš„æ¶æ„è®¾è®¡é—®é¢˜ï¼Œä½¿å…¶**æ— æ³•æ”¯æŒçœŸæ­£çš„å¹¶å‘æ“ä½œ**ã€‚æœ¬æ–‡å°†ç³»ç»Ÿæ€§åœ°åˆ†æè¿™äº›é™åˆ¶å¹¶æä¾›å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸš¨ æ ¸å¿ƒé—®é¢˜ï¼šå››é“æ— æ³•é€¾è¶Šçš„å¹¶å‘å±éšœ

LightRAG çš„å¹¶å‘é™åˆ¶æ¥è‡ªå››ä¸ªå±‚é¢ï¼Œæ¯ä¸€å±‚éƒ½è¶³ä»¥é˜»æ­¢çœŸæ­£çš„å¹¶å‘æ‰§è¡Œï¼š

### 1. å¼‚æ­¥åˆå§‹åŒ–ç¼ºé™·ï¼ˆğŸ”¥ æ ¹æœ¬æ€§é—®é¢˜ï¼‰

**é—®é¢˜æè¿°**ï¼šåœ¨å¼‚æ­¥ç¯å¢ƒä¸­åˆ›å»º LightRAG å®ä¾‹æ—¶ï¼Œä¼šè¿”å›ä¸€ä¸ªå°šæœªå®Œå…¨åˆå§‹åŒ–çš„å¯¹è±¡ã€‚

**æ ¸å¿ƒä»£ç åˆ†æ**ï¼š
```python
# lightrag/lightrag.py
def __post_init__(self):
    initialize_share_data()  # ä¿®æ”¹å…¨å±€çŠ¶æ€
    # ... å…¶ä»–åˆå§‹åŒ– ...
    if self.auto_manage_storages_states:  # é»˜è®¤ä¸ºTrue
        self._run_async_safely(self.initialize_storages, "Storage Initialization")

def _run_async_safely(self, async_func, action_name=""):
    loop = always_get_an_event_loop()
    if loop.is_running():
        # ğŸš« åˆ›å»ºåå°ä»»åŠ¡ä½†ä¸ç­‰å¾…å®Œæˆï¼
        task = loop.create_task(async_func())
        task.add_done_callback(lambda t: logger.info(f"{action_name} completed!"))
        # __post_init__ åœ¨è¿™é‡Œç›´æ¥è¿”å›ï¼Œå¯¹è±¡å¯èƒ½æœªå®Œå…¨åˆå§‹åŒ–
    else:
        # ä»…åœ¨åŒæ­¥ç¯å¢ƒä¸­ä¼šç­‰å¾…å®Œæˆ
        loop.run_until_complete(async_func())
```

**å¤±è´¥åœºæ™¯**ï¼š
```python
# âŒ åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼ˆå¦‚ Jupyterã€FastAPIã€æˆ– async å‡½æ•°ä¸­ï¼‰
async def problematic_usage():
    rag1 = LightRAG(working_dir="./rag1")  # è¿”å›æœªå®Œå…¨åˆå§‹åŒ–çš„å¯¹è±¡
    rag2 = LightRAG(working_dir="./rag2")  # åŒæ ·è¿”å›æœªå®Œå…¨åˆå§‹åŒ–çš„å¯¹è±¡
    
    # æ­¤æ—¶ä¸¤ä¸ªå®ä¾‹çš„ initialize_storages å¯èƒ½è¿˜åœ¨åå°è¿è¡Œ
    # ç«‹å³ä½¿ç”¨ä¼šå¯¼è‡´ä¸å¯é¢„æµ‹çš„é”™è¯¯
    await rag1.ainsert(["æµ‹è¯•æ–‡æ¡£"])  # å¯èƒ½å¤±è´¥
```

### 2. å…¨å±€çŠ¶æ€å†²çªï¼ˆğŸš« æ¶æ„æ€§é—®é¢˜ï¼‰

**é—®é¢˜æè¿°**ï¼šæ‰€æœ‰ LightRAG å®ä¾‹å…±äº«æ¨¡å—çº§å…¨å±€å˜é‡ï¼Œæ— æ³•å®ç°çœŸæ­£çš„éš”ç¦»ã€‚

**å…±äº«çš„å…¨å±€çŠ¶æ€**ï¼š
```python
# lightrag/kg/shared_storage.py - æ¨¡å—çº§å…¨å±€å˜é‡
_is_multiprocess = None
_manager = None 
_shared_dicts: Optional[Dict[str, Any]] = None
_pipeline_status_lock: Optional[LockType] = None
_storage_lock: Optional[LockType] = None
_graph_db_lock: Optional[LockType] = None
_initialized = None

def initialize_share_data(workers: int = 1):
    global _manager, _shared_dicts, _pipeline_status_lock, _initialized
    
    if _initialized:  # ç¬¬äºŒä¸ªå®ä¾‹é‡åˆ°è¿™ä¸ªæ£€æŸ¥
        direct_log("Shared-Data already initialized")
        return  # ä½†å¯èƒ½ä¸ç¬¦åˆç¬¬äºŒä¸ªå®ä¾‹çš„æœŸæœ›
    
    # åˆå§‹åŒ–å…¨å±€å…±äº«çŠ¶æ€ï¼ˆæ‰€æœ‰å®ä¾‹å…±äº«ï¼‰
    if workers > 1:
        _manager = Manager()
        _pipeline_status_lock = _manager.Lock()  # è¿›ç¨‹é—´å…±äº«é”
        _shared_dicts = _manager.dict()          # è¿›ç¨‹é—´å…±äº«å­—å…¸
    else:
        _pipeline_status_lock = asyncio.Lock()   # åŒè¿›ç¨‹å†…å…±äº«é”
        _shared_dicts = {}                       # åŒè¿›ç¨‹å†…å…±äº«å­—å…¸
```

**ç»“æœ**ï¼šå³ä½¿æ˜¯ä¸åŒ `working_dir` çš„å®ä¾‹ï¼Œä¹Ÿä¼šå…±äº«åŒä¸€å¥—å…¨å±€çŠ¶æ€ã€‚

### 3. å…¨å±€ç®¡é“äº’æ–¥é”ï¼ˆğŸš« è®¾è®¡æ€§é™åˆ¶ï¼‰

**é—®é¢˜æè¿°**ï¼š`pipeline_status["busy"]` ä½œä¸ºå…¨å±€äº’æ–¥æ ‡å¿—ï¼Œç¡®ä¿ä»»ä½•æ—¶å€™åªæœ‰ä¸€ä¸ª `ainsert` æ“ä½œèƒ½æ‰§è¡Œã€‚

**å…³é”®ä»£ç **ï¼š
```python
# lightrag/lightrag.py - apipeline_process_enqueue_documents
async def apipeline_process_enqueue_documents(self, ...):
    # è·å–å…¨å±€å…±äº«çš„ç®¡é“çŠ¶æ€
    pipeline_status = await get_namespace_data("pipeline_status")  # å…¨å±€å…±äº«
    pipeline_status_lock = get_pipeline_status_lock()              # å…¨å±€å…±äº«é”
    
    async with pipeline_status_lock:
        if not pipeline_status.get("busy", False):
            # ğŸ”¥ è®¾ç½®å…¨å±€å¿™ç¢Œæ ‡å¿—ï¼Œé˜»æ­¢æ‰€æœ‰å…¶ä»–å®ä¾‹
            pipeline_status["busy"] = True
            # å¼€å§‹å¤„ç†æ–‡æ¡£...
        else:
            # ğŸš« å…¶ä»–æ‰€æœ‰ ainsert è°ƒç”¨éƒ½ä¼šè¢«é˜»å¡
            pipeline_status["request_pending"] = True
            logger.info("Another process is already processing. Request queued.")
            return  # ç›´æ¥è¿”å›ï¼Œæ— æ³•å¹¶å‘ï¼
```

**ç»“æœ**ï¼šæ— è®ºåˆ›å»ºå¤šå°‘ä¸ª LightRAG å®ä¾‹ï¼ŒåŒæ—¶åªèƒ½æœ‰ä¸€ä¸ªåœ¨æ‰§è¡Œ `ainsert` æ“ä½œã€‚

### 4. å¤šè¿›ç¨‹å…±äº«é™åˆ¶ï¼ˆğŸš« æ— æ³•ç»•è¿‡ï¼‰

**é—®é¢˜æè¿°**ï¼šå³ä½¿ä½¿ç”¨å¤šè¿›ç¨‹ï¼ŒLightRAG ä»é€šè¿‡ `multiprocessing.Manager` å…±äº«çŠ¶æ€ã€‚

**ä»£ç åˆ†æ**ï¼š
```python
# å¤šè¿›ç¨‹æ¨¡å¼ä¸‹ä»ç„¶å…±äº«çŠ¶æ€
if workers > 1:
    _is_multiprocess = True
    _manager = Manager()                    # åˆ›å»ºè¿›ç¨‹é—´é€šä¿¡ç®¡ç†å™¨
    _pipeline_status_lock = _manager.Lock() # ğŸš« æ‰€æœ‰è¿›ç¨‹å…±äº«è¿™ä¸ªé”ï¼
    _shared_dicts = _manager.dict()         # ğŸš« æ‰€æœ‰è¿›ç¨‹å…±äº«è¿™ä¸ªå­—å…¸ï¼
```

**ç»“æœ**ï¼šå¤šè¿›ç¨‹ä»ç„¶å—å…¨å±€ `pipeline_status["busy"]` é™åˆ¶ï¼Œæ— æ³•å®ç°çœŸæ­£çš„å¹¶å‘ã€‚

## ğŸ“Š max_parallel_insert çš„çœŸå®ä½œç”¨

**å¸¸è§è¯¯è§£**ï¼šè®¤ä¸º `max_parallel_insert` æ§åˆ¶å¤šä¸ª `ainsert` è°ƒç”¨çš„å¹¶å‘ã€‚

**å®é™…æƒ…å†µ**ï¼š`max_parallel_insert` åªæ§åˆ¶**å•ä¸ª `ainsert` è°ƒç”¨å†…éƒ¨**çš„æ–‡æ¡£çº§å¹¶å‘ã€‚

**ä»£ç è¯æ®**ï¼š
```python
# åªæœ‰é€šè¿‡å…¨å±€äº’æ–¥æ£€æŸ¥åï¼Œmax_parallel_insert æ‰ç”Ÿæ•ˆ
async def apipeline_process_enqueue_documents(self, ...):
    # å‰é¢çš„å…¨å±€äº’æ–¥æ£€æŸ¥é€šè¿‡å...
    
    # ğŸ”¥ max_parallel_insert åªåœ¨è¿™é‡Œæ§åˆ¶å•ä¸ªæ‰¹æ¬¡å†…çš„æ–‡æ¡£å¹¶å‘
    semaphore = asyncio.Semaphore(self.max_parallel_insert)  # é»˜è®¤2
    
    async def process_document(doc_id, status_doc, semaphore, ...):
        async with semaphore:  # ğŸ“‹ æ–‡æ¡£çº§åˆ«å¹¶å‘æ§åˆ¶
            # å¤„ç†å•ä¸ªæ–‡æ¡£çš„æ‰€æœ‰ chunks
            chunks = self.chunking_func(...)
            await self._process_entity_relation_graph(chunks, ...)
    
    # ä¸ºå•ä¸ª ainsert è°ƒç”¨ä¸­çš„æ¯ä¸ªæ–‡æ¡£åˆ›å»ºä»»åŠ¡
    doc_tasks = []
    for doc_id, status_doc in to_process_docs.items():
        doc_tasks.append(process_document(doc_id, status_doc, semaphore, ...))
    
    await asyncio.gather(*doc_tasks)  # åœ¨å•ä¸ª ainsert å†…éƒ¨å¹¶å‘
```

**æ­£ç¡®ç†è§£**ï¼š
```python
# âœ… max_parallel_insert çš„å®é™…ä½œç”¨
await rag.ainsert([
    "document1",    # è¿™äº›æ–‡æ¡£åœ¨å•ä¸ª ainsert å†…éƒ¨
    "document2",    # å— max_parallel_insert=2 æ§åˆ¶  
    "document3",    # æœ€å¤š2ä¸ªæ–‡æ¡£åŒæ—¶å¤„ç†
    "document4",    # å…¶ä»–æ–‡æ¡£ç­‰å¾…å‰é¢çš„å®Œæˆ
])

# âŒ ä¸èƒ½æ§åˆ¶å¤šä¸ª ainsert è°ƒç”¨çš„å¹¶å‘
await asyncio.gather(
    rag1.ainsert(["doc1"]),  # ç¬¬ä¸€ä¸ªæ‰§è¡Œ
    rag2.ainsert(["doc2"])   # è¢«å…¨å±€é”é˜»å¡
)
```

## ğŸ¯ å®Œæ•´çš„å¹¶å‘é™åˆ¶æµç¨‹å›¾

```mermaid
graph TD
    A[åˆ›å»º LightRAG å®ä¾‹1] --> B[initialize_share_data åˆå§‹åŒ–å…¨å±€çŠ¶æ€]
    B --> C[_run_async_safely å¼‚æ­¥åˆå§‹åŒ–å­˜å‚¨]
    C --> D{äº‹ä»¶å¾ªç¯æ˜¯å¦è¿è¡Œ?}
    D -->|æ˜¯| E[åˆ›å»ºåå°ä»»åŠ¡ï¼Œç«‹å³è¿”å›æœªå®Œå…¨åˆå§‹åŒ–çš„å¯¹è±¡]
    D -->|å¦| F[ç­‰å¾…åˆå§‹åŒ–å®Œæˆ]
    
    G[åˆ›å»º LightRAG å®ä¾‹2] --> H{å…¨å±€çŠ¶æ€å·²åˆå§‹åŒ–?}
    H -->|æ˜¯| I[ä½¿ç”¨å·²æœ‰å…¨å±€çŠ¶æ€]
    H -->|å¦| J[é‡æ–°åˆå§‹åŒ–ï¼Œå¯èƒ½å†²çª]
    
    K[ainsert è°ƒç”¨1] --> L[è·å–å…¨å±€ pipeline_status]
    M[ainsert è°ƒç”¨2] --> L
    L --> N{pipeline_status.busy?}
    N -->|false| O[è®¾ç½® busy=Trueï¼Œå¼€å§‹å¤„ç†]
    N -->|true| P[è®¾ç½® request_pending=Trueï¼Œç›´æ¥è¿”å›]
    
    O --> Q[max_parallel_insert æ§åˆ¶æ–‡æ¡£å¹¶å‘]
    Q --> R[å¤„ç†å®Œæˆï¼Œè®¾ç½® busy=False]
    
    S[å¤šè¿›ç¨‹å°è¯•] --> T[Manager è¿›ç¨‹é—´å…±äº«]
    T --> L
```

## âœ… å¯è¡Œçš„è§£å†³æ–¹æ¡ˆ

åŸºäºä»¥ä¸Šåˆ†æï¼Œåªæœ‰ä»¥ä¸‹æ–¹æ¡ˆèƒ½å®ç°çœŸæ­£çš„å¹¶å‘ï¼š

### æ–¹æ¡ˆ1ï¼šå®Œå…¨éš”ç¦»çš„å­è¿›ç¨‹ï¼ˆæ¨èï¼‰

**åŸç†**ï¼šä¸ºæ¯ä¸ªä»»åŠ¡å¯åŠ¨ç‹¬ç«‹çš„ Python è§£é‡Šå™¨è¿›ç¨‹ï¼Œç¡®ä¿å®Œå…¨éš”ç¦»ã€‚

```python
import subprocess
import json
import tempfile
import concurrent.futures

class IsolatedLightRAG:
    """ä½¿ç”¨å®Œå…¨éš”ç¦»çš„å­è¿›ç¨‹è¿è¡Œ LightRAG"""
    
    def __init__(self, working_dir: str):
        self.working_dir = working_dir
    
    def process_documents(self, documents: list[str]) -> dict:
        """é€šè¿‡å­è¿›ç¨‹å¤„ç†æ–‡æ¡£"""
        # åˆ›å»ºå¤„ç†è„šæœ¬
        script = f'''
import asyncio
import json
from lightrag import LightRAG

async def main():
    rag = LightRAG(working_dir="{self.working_dir}")
    await rag.ainsert({json.dumps(documents)})
    return {{"status": "success", "count": {len(documents)}}}

result = asyncio.run(main())
print(json.dumps(result))
'''
        
        # å¯åŠ¨ç‹¬ç«‹çš„ Python è¿›ç¨‹
        result = subprocess.run([
            "python", "-c", script
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            return json.loads(result.stdout.strip())
        else:
            return {"status": "error", "error": result.stderr}

# ä½¿ç”¨ç¤ºä¾‹ï¼šçœŸæ­£çš„å¹¶å‘å¤„ç†
def parallel_processing():
    """å¤šä¸ªæ–‡æ¡£æ‰¹æ¬¡çš„å¹¶å‘å¤„ç†"""
    document_batches = [
        ["æ–‡æ¡£1", "æ–‡æ¡£2"],
        ["æ–‡æ¡£3", "æ–‡æ¡£4"], 
        ["æ–‡æ¡£5", "æ–‡æ¡£6"]
    ]
    
    rags = [
        IsolatedLightRAG(f"./rag_{i}") 
        for i in range(len(document_batches))
    ]
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œï¼ˆæ¯ä¸ªçº¿ç¨‹å¯åŠ¨ä¸€ä¸ªå­è¿›ç¨‹ï¼‰
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [
            executor.submit(rag.process_documents, docs)
            for rag, docs in zip(rags, document_batches)
        ]
        
        results = [future.result() for future in futures]
    
    return results

# è¿è¡Œå¹¶å‘å¤„ç†
results = parallel_processing()
print("å¹¶å‘å¤„ç†ç»“æœ:", results)
```

### æ–¹æ¡ˆ2ï¼šåŸºäºæ¶ˆæ¯é˜Ÿåˆ—çš„ä»»åŠ¡åˆ†å‘ï¼ˆç”Ÿäº§çº§ï¼‰

**åŸç†**ï¼šä½¿ç”¨ Celery ç­‰ä»»åŠ¡é˜Ÿåˆ—ï¼Œæ¯ä¸ª worker è¿è¡Œåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­ã€‚

```python
from celery import Celery
import asyncio

app = Celery('lightrag_tasks', broker='redis://localhost:6379')

@app.task
def process_documents_task(documents, working_dir):
    """åœ¨ç‹¬ç«‹çš„ Celery worker ä¸­å¤„ç†æ–‡æ¡£"""
    def run_lightrag():
        from lightrag import LightRAG
        
        rag = LightRAG(working_dir=working_dir)
        
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯ï¼ˆé¿å…ä¸ Celery å†²çªï¼‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            result = loop.run_until_complete(rag.ainsert(documents))
            return {"status": "success", "working_dir": working_dir}
        except Exception as e:
            return {"status": "error", "error": str(e)}
        finally:
            loop.close()
    
    return run_lightrag()

# ä½¿ç”¨ç¤ºä¾‹
def distribute_processing():
    """åˆ†å‘ä»»åŠ¡åˆ°å¤šä¸ª Celery worker"""
    document_batches = [
        ["æ–‡æ¡£1", "æ–‡æ¡£2"],
        ["æ–‡æ¡£3", "æ–‡æ¡£4"],
        ["æ–‡æ¡£5", "æ–‡æ¡£6"]
    ]
    
    # æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
    jobs = []
    for i, docs in enumerate(document_batches):
        job = process_documents_task.delay(docs, f"./rag_{i}")
        jobs.append(job)
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    results = [job.get() for job in jobs]
    return results
```

### æ–¹æ¡ˆ3ï¼šè‡ªå®ç°æ— çŠ¶æ€ç‰ˆæœ¬ï¼ˆé•¿æœŸæ–¹æ¡ˆï¼‰

**åŸç†**ï¼šæå– LightRAG çš„æ ¸å¿ƒç®—æ³•ï¼Œé‡æ–°å®ç°ä¸ºæ— å…¨å±€çŠ¶æ€çš„ç‰ˆæœ¬ã€‚

```python
import asyncio
from typing import List, Dict, Any

class StatelessLightRAG:
    """æ— å…¨å±€çŠ¶æ€çš„ LightRAG å®ç°"""
    
    def __init__(self, working_dir: str):
        self.working_dir = working_dir
        # æ‰€æœ‰çŠ¶æ€éƒ½æ˜¯å®ä¾‹æœ¬åœ°çš„ï¼Œæ— å…¨å±€ä¾èµ–
        self.tokenizer = self._init_tokenizer()
        self.llm_func = self._init_llm()
        self.embedding_func = self._init_embedding()
    
    async def ainsert(self, documents: List[str]) -> Dict[str, Any]:
        """å®Œå…¨æ— çŠ¶æ€çš„æ–‡æ¡£æ’å…¥"""
        results = []
        
        # å¹¶å‘å¤„ç†å¤šä¸ªæ–‡æ¡£ï¼ˆæ— å…¨å±€çŠ¶æ€é™åˆ¶ï¼‰
        tasks = [self._process_document(doc) for doc in documents]
        doc_results = await asyncio.gather(*tasks)
        
        return {
            "results": doc_results, 
            "total_documents": len(documents)
        }
    
    async def _process_document(self, document: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        # 1. æ–‡æ¡£åˆ†å—
        chunks = self._chunk_document(document)
        
        # 2. å¹¶å‘å®ä½“æŠ½å–ï¼ˆæ— å…¨å±€çŠ¶æ€é™åˆ¶ï¼‰
        entity_tasks = [
            self._extract_entities_from_chunk(chunk) 
            for chunk in chunks
        ]
        chunk_results = await asyncio.gather(*entity_tasks)
        
        # 3. å®ä½“å…³ç³»åˆå¹¶
        merged_entities = self._merge_entities(chunk_results)
        
        # 4. å­˜å‚¨åˆ°æœ¬åœ°ï¼ˆæ— å…±äº«çŠ¶æ€ï¼‰
        await self._store_to_local(merged_entities, document)
        
        return {
            "document_summary": document[:100] + "...",
            "entities_count": len(merged_entities),
            "status": "success"
        }
    
    # å®ç°æ ¸å¿ƒç®—æ³•ä½†æ— å…¨å±€çŠ¶æ€ä¾èµ–
    def _chunk_document(self, content: str) -> List[Dict[str, Any]]:
        """å¤ç”¨ LightRAG çš„åˆ†å—é€»è¾‘"""
        pass
    
    async def _extract_entities_from_chunk(self, chunk: Dict[str, Any]):
        """å¤ç”¨ LightRAG çš„å®ä½“æŠ½å–é€»è¾‘"""
        pass
    
    def _merge_entities(self, chunk_results: List[Dict[str, Any]]):
        """å¤ç”¨ LightRAG çš„å®ä½“åˆå¹¶é€»è¾‘"""
        pass

# çœŸæ­£å¹¶å‘çš„ä½¿ç”¨ç¤ºä¾‹
async def truly_concurrent_processing():
    """åŒæ—¶å¤„ç†å¤šä¸ªæ–‡æ¡£é›†åˆ"""
    rags = [
        StatelessLightRAG(f"./stateless_rag_{i}")
        for i in range(3)
    ]
    
    document_batches = [
        ["æ–‡æ¡£1", "æ–‡æ¡£2"],
        ["æ–‡æ¡£3", "æ–‡æ¡£4"],
        ["æ–‡æ¡£5", "æ–‡æ¡£6"]
    ]
    
    # çœŸæ­£çš„å¹¶å‘æ‰§è¡Œï¼ˆæ— å…¨å±€çŠ¶æ€é™åˆ¶ï¼‰
    tasks = [
        rag.ainsert(docs) 
        for rag, docs in zip(rags, document_batches)
    ]
    
    results = await asyncio.gather(*tasks)
    return results

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    results = asyncio.run(truly_concurrent_processing())
    print("çœŸæ­£å¹¶å‘çš„å¤„ç†ç»“æœ:", results)
```

## ğŸ“‹ æ€»ç»“

### LightRAG çš„æ ¹æœ¬æ€§é™åˆ¶

1. **ğŸ”¥ å¼‚æ­¥åˆå§‹åŒ–ç¼ºé™·**ï¼šåœ¨å¼‚æ­¥ç¯å¢ƒä¸­è¿”å›æœªå®Œå…¨åˆå§‹åŒ–çš„å¯¹è±¡
2. **ğŸš« å…¨å±€çŠ¶æ€å†²çª**ï¼šæ‰€æœ‰å®ä¾‹å…±äº«æ¨¡å—çº§å…¨å±€å˜é‡ï¼Œæ— æ³•éš”ç¦»
3. **ğŸš« å…¨å±€ç®¡é“äº’æ–¥**ï¼š`pipeline_status["busy"]` ç¡®ä¿åŒæ—¶åªæœ‰ä¸€ä¸ª `ainsert` æ‰§è¡Œ
4. **ğŸš« å¤šè¿›ç¨‹å…±äº«é™åˆ¶**ï¼šManager æœºåˆ¶ä½¿å¤šè¿›ç¨‹ä¹Ÿå—å…¨å±€çŠ¶æ€é™åˆ¶

### å…³é”®æ´å¯Ÿ

- **è®¾è®¡å“²å­¦**ï¼šLightRAG é’ˆå¯¹**å•å®ä¾‹ã€å•ç”¨æˆ·ã€æ‰¹é‡å¤„ç†**åœºæ™¯è®¾è®¡
- **å‚æ•°è¯¯è§£**ï¼š`max_parallel_insert` ä¸æ§åˆ¶å¤šä¸ª `ainsert` çš„å¹¶å‘
- **æ¶æ„ç¼ºé™·**ï¼šä»äº‹ä»¶å¾ªç¯ç®¡ç†åˆ°å…¨å±€çŠ¶æ€è®¾è®¡éƒ½å­˜åœ¨å¹¶å‘éšœç¢

### æ¨èæ–¹æ¡ˆ

- **çŸ­æœŸæ–¹æ¡ˆ**ï¼šä½¿ç”¨å®Œå…¨éš”ç¦»çš„å­è¿›ç¨‹
- **ç”Ÿäº§æ–¹æ¡ˆ**ï¼šåŸºäº Celery ç­‰æ¶ˆæ¯é˜Ÿåˆ—çš„ä»»åŠ¡åˆ†å‘
- **é•¿æœŸæ–¹æ¡ˆ**ï¼šåŸºäº LightRAG ç®—æ³•é‡æ–°å®ç°æ— å…¨å±€çŠ¶æ€ç‰ˆæœ¬

**æ ¸å¿ƒå»ºè®®**ï¼šå¦‚æœéœ€è¦é«˜å¹¶å‘å¤„ç†ï¼Œå°† LightRAG è§†ä¸ºç®—æ³•å‚è€ƒï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨å¹¶å‘åœºæ™¯ä¸­ä½¿ç”¨ã€‚è®¾è®¡é€‚åˆçš„æ¶æ„æ¥ç»•è¿‡å…¶å›ºæœ‰çš„é™åˆ¶ï¼Œæˆ–è€ƒè™‘é‡æ–°å®ç°å…¶æ ¸å¿ƒåŠŸèƒ½ã€‚