# LightRAGæ ¸å¿ƒæŠ€æœ¯åˆ†ææ–‡æ¡£

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æLightRAGçš„å†™å…¥(Index)å’ŒæŸ¥è¯¢(Retrieval)æµç¨‹çš„æ ¸å¿ƒæŠ€æœ¯å®ç°ï¼ŒåŒ…æ‹¬å…·ä½“çš„ç®—æ³•ã€å·¥å…·ã€promptæ¨¡æ¿å’Œæ•°æ®ç»“æ„ã€‚

## ä¸€ã€å†™å…¥æµç¨‹æŠ€æœ¯åˆ†æ

### 1.1 æ–‡æ¡£åˆ†å—(Chunking)æŠ€æœ¯

**æ ¸å¿ƒå‡½æ•°**: `chunking_by_token_size`

**ä½¿ç”¨å·¥å…·**:
- **tiktoken**: OpenAIçš„tokenizeråº“ï¼Œç”¨äºç²¾ç¡®è®¡ç®—tokenæ•°é‡
- **é»˜è®¤æ¨¡å‹**: `gpt-4o-mini` (å¯é…ç½®)

**æŠ€æœ¯å‚æ•°**:
```python
chunk_token_size: int = 1200        # é»˜è®¤æ¯å—æœ€å¤§tokenæ•°
chunk_overlap_token_size: int = 100  # å—é—´é‡å tokenæ•°
```

**ç®—æ³•é€»è¾‘**:
1. **åŸºç¡€åˆ†å—**: æŒ‰`max_token_size - overlap_token_size`æ­¥é•¿æ»‘åŠ¨çª—å£åˆ†å—
2. **å­—ç¬¦åˆ†å‰²**: æ”¯æŒæŒ‰ç‰¹å®šå­—ç¬¦(å¦‚`\n\n`)é¢„åˆ†å‰²
3. **æ··åˆç­–ç•¥**: å…ˆæŒ‰å­—ç¬¦åˆ†å‰²ï¼Œè¶…é•¿çš„å†æŒ‰tokenåˆ†å‰²
4. **é‡å ä¿æŒ**: ç¡®ä¿å—é—´æœ‰overlap_token_sizeä¸ªtokené‡å ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§

**è¾“å‡ºæ•°æ®ç»“æ„**:
```python
{
    "tokens": int,              # è¯¥å—çš„tokenæ•°é‡
    "content": str,             # å—å†…å®¹
    "chunk_order_index": int,   # å—åœ¨æ–‡æ¡£ä¸­çš„é¡ºåº
    "full_doc_id": str,         # æ‰€å±æ–‡æ¡£ID
    "file_path": str            # æ–‡ä»¶è·¯å¾„(ç”¨äºå¼•ç”¨)
}
```

### 1.2 å®ä½“æŠ½å–æŠ€æœ¯

**æ ¸å¿ƒå‡½æ•°**: `extract_entities` + `_handle_single_entity_extraction`

**LLMè°ƒç”¨ç­–ç•¥**:
- **å¹¶å‘æ§åˆ¶**: `llm_model_max_async = 4` (é»˜è®¤)
- **é‡è¯•æœºåˆ¶**: `entity_extract_max_gleaning = 1` (æœ€å¤§è¡¥å……æŠ½å–æ¬¡æ•°)
- **ç¼“å­˜ç³»ç»Ÿ**: æ”¯æŒLLMå“åº”ç¼“å­˜é¿å…é‡å¤è®¡ç®—

**æ ¸å¿ƒPromptæ¨¡æ¿** (`entity_extraction`):
```
---Goal---
Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.
Use {language} as output language.

---Steps---
1. Identify all entities. For each identified entity, extract the following information:
- entity_name: Name of the entity, use same language as input text. If English, capitalized the name.
- entity_type: One of the following types: [{entity_types}]
- entity_description: Comprehensive description of the entity's attributes and activities
Format each entity as ("entity"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>)

2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.
For each pair of related entities, extract the following information:
- source_entity: name of the source entity, as identified in step 1
- target_entity: name of the target entity, as identified in step 1  
- relationship_description: explanation as to why you think the source entity and the target entity are related to each other
- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity
- relationship_keywords: one or more high-level key words that summarize the overarching nature of the relationship, focusing on concepts or themes rather than specific details
Format each relationship as ("relationship"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_keywords>{tuple_delimiter}<relationship_strength>)

3. Identify high-level key words that summarize the main concepts, themes, or topics of the entire text. These should capture the overarching ideas present in the document.
Format the content-level key words as ("content_keywords"{tuple_delimiter}<high_level_keywords>)

4. Return output in {language} as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.

5. When finished, output {completion_delimiter}
```

**åˆ†éš”ç¬¦é…ç½®**:
```python
PROMPTS["DEFAULT_TUPLE_DELIMITER"] = "<|>"
PROMPTS["DEFAULT_RECORD_DELIMITER"] = "##"
PROMPTS["DEFAULT_COMPLETION_DELIMITER"] = "<|COMPLETE|>"
```

**é»˜è®¤å®ä½“ç±»å‹**:
```python
PROMPTS["DEFAULT_ENTITY_TYPES"] = ["organization", "person", "geo", "event", "category"]
```

**è¡¥å……æŠ½å–æœºåˆ¶**:
- **ç»§ç»­æå–Prompt** (`entity_continue_extraction`): æç¤ºLLMè¡¥å……é—æ¼çš„å®ä½“å’Œå…³ç³»
- **å¾ªç¯æ£€æŸ¥Prompt** (`entity_if_loop_extraction`): è¯¢é—®æ˜¯å¦è¿˜æœ‰é—æ¼ï¼Œè¿”å›YES/NO

#### 1.2.1 å…³ç³»æŠ½å–æŠ€æœ¯è¯¦æ

**æ ¸å¿ƒå‡½æ•°**: `_handle_single_relationship_extraction`

**å…³ç³»æŠ½å–ç®—æ³•**:
```python
async def _handle_single_relationship_extraction(
    record_attributes: list[str],
    chunk_key: str,
    file_path: str = "unknown_source",
):
    # 1. éªŒè¯æ ¼å¼ï¼šå¿…é¡»åŒ…å«"relationship"æ ‡è¯†ç¬¦ä¸”è‡³å°‘5ä¸ªå­—æ®µ
    if len(record_attributes) < 5 or '"relationship"' not in record_attributes[0]:
        return None
        
    # 2. æå–å¹¶æ¸…ç†æºå®ä½“å’Œç›®æ ‡å®ä½“åç§°
    source = clean_str(record_attributes[1])  # æºå®ä½“
    target = clean_str(record_attributes[2])  # ç›®æ ‡å®ä½“
    
    # 3. å®ä½“åç§°æ ‡å‡†åŒ–
    source = normalize_extracted_info(source, is_entity=True)
    target = normalize_extracted_info(target, is_entity=True)
    
    # 4. è‡ªç¯æ£€æµ‹ï¼šæºå®ä½“å’Œç›®æ ‡å®ä½“ç›¸åŒåˆ™ä¸¢å¼ƒ
    if source == target:
        return None
        
    # 5. å…³ç³»æè¿°å¤„ç†
    edge_description = clean_str(record_attributes[3])
    edge_description = normalize_extracted_info(edge_description)
    
    # 6. å…³ç³»å…³é”®è¯å¤„ç†
    edge_keywords = normalize_extracted_info(
        clean_str(record_attributes[4]), is_entity=True
    )
    edge_keywords = edge_keywords.replace("ï¼Œ", ",")  # ä¸­æ–‡é€—å·è½¬è‹±æ–‡
    
    # 7. å…³ç³»æƒé‡æå–ï¼ˆé»˜è®¤1.0ï¼‰
    weight = float(record_attributes[-1]) if is_float_regex(record_attributes[-1]) else 1.0
    
    return {
        "src_id": source,
        "tgt_id": target, 
        "weight": weight,
        "description": edge_description,
        "keywords": edge_keywords,
        "source_id": chunk_key,  # æ¥æºchunkæ ‡è¯†
        "file_path": file_path   # æ–‡ä»¶è·¯å¾„
    }
```

**å…³ç³»æ•°æ®ç»“æ„**:
```python
{
    "src_id": str,          # æºå®ä½“åç§°
    "tgt_id": str,          # ç›®æ ‡å®ä½“åç§°
    "weight": float,        # å…³ç³»æƒé‡(é»˜è®¤1.0)
    "description": str,     # å…³ç³»æè¿°
    "keywords": str,        # å…³ç³»å…³é”®è¯(é€—å·åˆ†éš”)
    "source_id": str,       # æ¥æºchunk ID
    "file_path": str        # æ–‡ä»¶è·¯å¾„
}
```

**å…³ç³»å‘é‡åŒ–ç­–ç•¥**:
```python
# å…³ç³»å†…å®¹æ„å»ºç”¨äºå‘é‡åŒ–
relationship_content = keywords + src_id + tgt_id + description
```

### 1.3 å®ä½“å’Œå…³ç³»åˆå¹¶æŠ€æœ¯

**å®ä½“åˆå¹¶** (`_merge_nodes_then_upsert`):

**å»é‡ç­–ç•¥**:
- **å®ä½“ç±»å‹**: é€‰æ‹©å‡ºç°é¢‘æ¬¡æœ€é«˜çš„ç±»å‹
- **æè¿°åˆå¹¶**: ç”¨`GRAPH_FIELD_SEP`è¿æ¥æ‰€æœ‰æè¿°
- **æ¥æºè¿½è¸ª**: è®°å½•æ‰€æœ‰æ¥æºchunk_idå’Œfile_path

**LLMæ€»ç»“è§¦å‘**:
```python
force_llm_summary_on_merge: int = 10  # é»˜è®¤å€¼
```
å½“æè¿°ç‰‡æ®µæ•°é‡ >= 10æ—¶ï¼Œè‡ªåŠ¨è°ƒç”¨LLMè¿›è¡Œæ€»ç»“

**æ€»ç»“Prompt** (`summarize_entity_descriptions`):
```
You are a helpful assistant responsible for generating a comprehensive summary of the data provided below.
Given one or two entities, and a list of descriptions, all related to the same entity or group of entities.
Please concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.
If the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.
Make sure it is written in third person, and include the entity names so we the have full context.
Use {language} as output language.

#######
---Data---
Entities: {entity_name}
Description List: {description_list}
#######
Output:
```

**å…³ç³»åˆå¹¶** (`_merge_edges_then_upsert`):
- **æƒé‡ç´¯åŠ **: æ‰€æœ‰ç›¸åŒå…³ç³»çš„æƒé‡ç´¯åŠ 
- **å…³é”®è¯åˆå¹¶**: å»é‡åç”¨é€—å·è¿æ¥
- **æè¿°åˆå¹¶**: ç±»ä¼¼å®ä½“æè¿°åˆå¹¶ç­–ç•¥

### 1.4 å­˜å‚¨æŠ€æœ¯æ¶æ„

**å‘é‡å­˜å‚¨**:
```python
# å®ä½“å‘é‡åŒ–
entities_vdb: {
    "content": entity_name + "\n" + description,
    "entity_name": str,
    "source_id": str,
    "description": str, 
    "entity_type": str,
    "file_path": str
}

# å…³ç³»å‘é‡åŒ–  
relationships_vdb: {
    "content": keywords + src_id + tgt_id + description,
    "src_id": str,
    "tgt_id": str,
    "source_id": str,
    "description": str,
    "keywords": str,
    "file_path": str
}

# æ–‡æ¡£å—å‘é‡åŒ–
chunks_vdb: {
    "content": chunk_content,
    "full_doc_id": str,
    "file_path": str,
    "tokens": int,
    "chunk_order_index": int
}
```

**å›¾å­˜å‚¨**: 
- **èŠ‚ç‚¹å±æ€§**: entity_id, entity_type, description, source_id, file_path, created_at
- **è¾¹å±æ€§**: weight, description, keywords, source_id, file_path, created_at

**é”®å€¼å­˜å‚¨**:
- **å…¨æ–‡æ–‡æ¡£**: full_docså­˜å‚¨å®Œæ•´æ–‡æ¡£å†…å®¹
- **æ–‡æ¡£å—**: text_chunkså­˜å‚¨åˆ†å—æ•°æ®
- **LLMç¼“å­˜**: ç¼“å­˜å®ä½“æŠ½å–å’Œæ€»ç»“çš„LLMå“åº”

## äºŒã€æŸ¥è¯¢æµç¨‹æŠ€æœ¯åˆ†æ

### 2.1 å…³é”®è¯æå–æŠ€æœ¯

**æ ¸å¿ƒå‡½æ•°**: `extract_keywords_only`

**å…³é”®è¯æå–Prompt** (`keywords_extraction`):
```
---Role---
You are a helpful assistant tasked with identifying both high-level and low-level keywords in the user's query and conversation history.

---Goal---
Given the query and conversation history, list both high-level and low-level keywords. High-level keywords focus on overarching concepts or themes, while low-level keywords focus on specific entities, details, or concrete terms.

---Instructions---
- Consider both the current query and relevant conversation history when extracting keywords
- Output the keywords in JSON format, it will be parsed by a JSON parser, do not add any extra content in output
- The JSON should have two keys:
  - "high_level_keywords" for overarching concepts or themes
  - "low_level_keywords" for specific entities or details
```

**è¾“å‡ºæ ¼å¼**:
```json
{
  "high_level_keywords": ["æ¦‚å¿µ", "ä¸»é¢˜"],
  "low_level_keywords": ["å…·ä½“å®ä½“", "ç»†èŠ‚"]
}
```

#### 2.1.1 å…³é”®è¯æå–è¯¦ç»†æ­¥éª¤

**å®Œæ•´ç®—æ³•æµç¨‹**:

```python
async def extract_keywords_only(
    text: str,
    param: QueryParam,
    global_config: dict[str, str],
    hashing_kv: BaseKVStorage | None = None,
) -> tuple[list[str], list[str]]:
    
    # æ­¥éª¤1: ç¼“å­˜æ£€æŸ¥
    args_hash = compute_args_hash(param.mode, text, cache_type="keywords")
    cached_response, quantized, min_val, max_val = await handle_cache(
        hashing_kv, args_hash, text, param.mode, cache_type="keywords"
    )
    if cached_response is not None:
        # ä»ç¼“å­˜è¿”å›è§£æåçš„å…³é”®è¯
        keywords_data = json.loads(cached_response)
        return keywords_data["high_level_keywords"], keywords_data["low_level_keywords"]
    
    # æ­¥éª¤2: æ„å»ºç¤ºä¾‹ä¸Šä¸‹æ–‡
    example_number = global_config["addon_params"].get("example_number", None)
    if example_number and example_number < len(PROMPTS["keywords_extraction_examples"]):
        examples = "\n".join(PROMPTS["keywords_extraction_examples"][:int(example_number)])
    else:
        examples = "\n".join(PROMPTS["keywords_extraction_examples"])
    
    # æ­¥éª¤3: å¤„ç†å¯¹è¯å†å²
    history_context = ""
    if param.conversation_history:
        history_context = get_conversation_turns(
            param.conversation_history, param.history_turns
        )
    
    # æ­¥éª¤4: æ„å»ºå…³é”®è¯æå–prompt
    language = global_config["addon_params"].get("language", "English")
    kw_prompt = PROMPTS["keywords_extraction"].format(
        query=text, 
        examples=examples, 
        language=language, 
        history=history_context
    )
    
    # æ­¥éª¤5: LLMè°ƒç”¨
    use_model_func = param.model_func or global_config["llm_model_func"]
    result = await use_model_func(kw_prompt, keyword_extraction=True)
    
    # æ­¥éª¤6: JSONè§£æ
    match = re.search(r"\{.*\}", result, re.DOTALL)
    if not match:
        return [], []  # è§£æå¤±è´¥è¿”å›ç©ºåˆ—è¡¨
        
    try:
        keywords_data = json.loads(match.group(0))
    except json.JSONDecodeError:
        return [], []
    
    hl_keywords = keywords_data.get("high_level_keywords", [])
    ll_keywords = keywords_data.get("low_level_keywords", [])
    
    # æ­¥éª¤7: ç»“æœç¼“å­˜
    if hl_keywords or ll_keywords:
        cache_data = {
            "high_level_keywords": hl_keywords,
            "low_level_keywords": ll_keywords,
        }
        if hashing_kv.global_config.get("enable_llm_cache"):
            await save_to_cache(hashing_kv, CacheData(...))
    
    return hl_keywords, ll_keywords
```

**å…³é”®è¯åˆ†ç±»ç­–ç•¥**:
- **High-level keywords**: æŠ½è±¡æ¦‚å¿µã€ä¸»é¢˜ã€é¢†åŸŸæœ¯è¯­
- **Low-level keywords**: å…·ä½“å®ä½“ã€äººåã€åœ°åã€äº§å“å

### 2.2 æŸ¥è¯¢æ¨¡å¼æŠ€æœ¯

**ä¸‰ç§æŸ¥è¯¢æ¨¡å¼**:

1. **Localæ¨¡å¼**: åŸºäºå…·ä½“å®ä½“çš„å±€éƒ¨æŸ¥è¯¢
   - ä½¿ç”¨`ll_keywords`åœ¨`entities_vdb`ä¸­å‘é‡æœç´¢
   - è·å–ç›¸å…³å®ä½“çš„é‚»å±…èŠ‚ç‚¹å’Œè¾¹
   - é€‚ç”¨äºå…·ä½“å®ä½“æŸ¥è¯¢

2. **Globalæ¨¡å¼**: åŸºäºæ¦‚å¿µä¸»é¢˜çš„å…¨å±€æŸ¥è¯¢
   - ä½¿ç”¨`hl_keywords`åœ¨`relationships_vdb`ä¸­å‘é‡æœç´¢
   - è·å–ç›¸å…³å…³ç³»åŠå…¶è¿æ¥çš„å®ä½“
   - é€‚ç”¨äºæ¦‚å¿µæ€§æŸ¥è¯¢

3. **Hybridæ¨¡å¼**: æ··åˆæŸ¥è¯¢
   - åŒæ—¶æ‰§è¡ŒLocalå’ŒGlobalæŸ¥è¯¢
   - åˆå¹¶å»é‡ç»“æœ
   - é€‚ç”¨äºå¤åˆæŸ¥è¯¢

### 2.3 ä¸Šä¸‹æ–‡æ„å»ºæŠ€æœ¯

**Localæ¨¡å¼ä¸Šä¸‹æ–‡æ„å»º** (`_get_node_data`):

**æ­¥éª¤1**: å®ä½“å‘é‡æœç´¢
```python
results = await entities_vdb.query(query, top_k=query_param.top_k)
```

**æ­¥éª¤2**: æ‰¹é‡è·å–å®ä½“è¯¦æƒ…å’Œåº¦æ•°
```python
nodes_dict, degrees_dict = await asyncio.gather(
    knowledge_graph_inst.get_nodes_batch(node_ids),
    knowledge_graph_inst.node_degrees_batch(node_ids)
)
```

**æ­¥éª¤3**: æŸ¥æ‰¾ç›¸å…³æ–‡æœ¬å—
- ä»å®ä½“çš„`source_id`å­—æ®µè·å–ç›¸å…³chunk_id
- è·å–ä¸€è·³é‚»å±…å®ä½“çš„æ–‡æœ¬å—
- æŒ‰relation_countsæ’åºä¼˜åŒ–

**æ­¥éª¤4**: æŸ¥æ‰¾ç›¸å…³è¾¹
- è·å–æ‰€æœ‰é€‰ä¸­å®ä½“çš„è¾¹
- æŒ‰åº¦æ•°å’Œæƒé‡æ’åº
- Tokené™åˆ¶æˆªæ–­

**Globalæ¨¡å¼ä¸Šä¸‹æ–‡æ„å»º** (`_get_edge_data`):

**æ­¥éª¤1**: å…³ç³»å‘é‡æœç´¢
```python
results = await relationships_vdb.query(keywords, top_k=query_param.top_k)
```

**æ­¥éª¤2**: æ‰¹é‡è·å–è¾¹è¯¦æƒ…å’Œåº¦æ•°
```python
edge_data_dict, edge_degrees_dict = await asyncio.gather(
    knowledge_graph_inst.get_edges_batch(edge_pairs_dicts),
    knowledge_graph_inst.edge_degrees_batch(edge_pairs_tuples)
)
```

**æ­¥éª¤3**: ä»å…³ç³»åå‘æŸ¥æ‰¾ç›¸å…³å®ä½“å’Œæ–‡æœ¬å—

#### 2.3.1 LocalæŸ¥è¯¢è¯¦ç»†ç®—æ³•

**å®Œæ•´LocalæŸ¥è¯¢æµç¨‹** (`_get_node_data`):

```python
async def _get_node_data(
    query: str,  # è¿™é‡Œçš„queryå®é™…æ˜¯ll_keywordså­—ç¬¦ä¸²
    knowledge_graph_inst: BaseGraphStorage,
    entities_vdb: BaseVectorStorage,
    text_chunks_db: BaseKVStorage,
    query_param: QueryParam,
):
    # ç¬¬1æ­¥: åŸºäºlow-level keywordsè¿›è¡Œå®ä½“å‘é‡æœç´¢
    results = await entities_vdb.query(
        query,  # ll_keywordsä½œä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²
        top_k=query_param.top_k, 
        ids=query_param.ids
    )
    
    if not len(results):
        return "", "", ""  # æ— åŒ¹é…å®ä½“æ—¶è¿”å›ç©ºç»“æœ
    
    # ç¬¬2æ­¥: æå–å®ä½“IDåˆ—è¡¨å¹¶æ‰¹é‡è·å–å®ä½“æ•°æ®
    node_ids = [r["entity_name"] for r in results]
    
    # å¹¶å‘è·å–å®ä½“è¯¦æƒ…å’Œå›¾ä¸­åº¦æ•°
    nodes_dict, degrees_dict = await asyncio.gather(
        knowledge_graph_inst.get_nodes_batch(node_ids),
        knowledge_graph_inst.node_degrees_batch(node_ids)
    )
    
    # ç¬¬3æ­¥: æ„å»ºå¸¦æœ‰åº¦æ•°ä¿¡æ¯çš„å®ä½“æ•°æ®
    node_datas = [
        {
            **n,  # å®ä½“è¯¦ç»†ä¿¡æ¯
            "entity_name": k["entity_name"],
            "rank": d,  # å›¾ä¸­åº¦æ•°ä½œä¸ºé‡è¦æ€§æ’å
            "created_at": k.get("created_at"),
        }
        for k, n, d in zip(results, [nodes_dict.get(nid) for nid in node_ids], 
                           [degrees_dict.get(nid, 0) for nid in node_ids])
        if n is not None
    ]
    
    # ç¬¬4æ­¥: æŸ¥æ‰¾ç›¸å…³æ–‡æœ¬å•å…ƒ
    use_text_units = await _find_most_related_text_unit_from_entities(
        node_datas, query_param, text_chunks_db, knowledge_graph_inst
    )
    
    # ç¬¬5æ­¥: æŸ¥æ‰¾ç›¸å…³å…³ç³»è¾¹
    use_relations = await _find_most_related_edges_from_entities(
        node_datas, query_param, knowledge_graph_inst
    )
    
    # ç¬¬6æ­¥: Tokené¢„ç®—ç®¡ç† - æˆªæ–­å®ä½“åˆ—è¡¨
    tokenizer = text_chunks_db.global_config.get("tokenizer")
    len_node_datas = len(node_datas)
    node_datas = truncate_list_by_token_size(
        node_datas,
        key=lambda x: x["description"] if x["description"] is not None else "",
        max_token_size=query_param.max_token_for_local_context,
        tokenizer=tokenizer,
    )
    
    # ç¬¬7æ­¥: æ„å»ºæ ‡å‡†åŒ–çš„å®ä½“ä¸Šä¸‹æ–‡
    entities_context = []
    for i, n in enumerate(node_datas):
        created_at = n.get("created_at", "UNKNOWN")
        if isinstance(created_at, (int, float)):
            created_at = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(created_at))
        
        entities_context.append({
            "id": i + 1,
            "entity": n["entity_name"],
            "type": n.get("entity_type", "UNKNOWN"),
            "description": n.get("description", "UNKNOWN"),
            "rank": n["rank"],
            "created_at": created_at,
            "file_path": n.get("file_path", "unknown_source"),
        })
    
    # ç¬¬8æ­¥: æ„å»ºå…³ç³»ä¸Šä¸‹æ–‡å’Œæ–‡æœ¬ä¸Šä¸‹æ–‡...
    return entities_context, relations_context, text_units_context
```

#### 2.3.2 GlobalæŸ¥è¯¢è¯¦ç»†ç®—æ³•  

**å®Œæ•´GlobalæŸ¥è¯¢æµç¨‹** (`_get_edge_data`):

```python
async def _get_edge_data(
    keywords,  # è¿™é‡Œæ˜¯hl_keywordså­—ç¬¦ä¸²
    knowledge_graph_inst: BaseGraphStorage,
    relationships_vdb: BaseVectorStorage,
    text_chunks_db: BaseKVStorage,
    query_param: QueryParam,
):
    # ç¬¬1æ­¥: åŸºäºhigh-level keywordsè¿›è¡Œå…³ç³»å‘é‡æœç´¢
    results = await relationships_vdb.query(
        keywords,  # hl_keywordsä½œä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²
        top_k=query_param.top_k, 
        ids=query_param.ids
    )
    
    if not len(results):
        return "", "", ""
    
    # ç¬¬2æ­¥: å‡†å¤‡è¾¹çš„æ‰¹é‡æŸ¥è¯¢æ•°æ®
    edge_pairs_dicts = [{"src": r["src_id"], "tgt": r["tgt_id"]} for r in results]
    edge_pairs_tuples = [(r["src_id"], r["tgt_id"]) for r in results]
    
    # å¹¶å‘è·å–è¾¹è¯¦æƒ…å’Œåº¦æ•°
    edge_data_dict, edge_degrees_dict = await asyncio.gather(
        knowledge_graph_inst.get_edges_batch(edge_pairs_dicts),
        knowledge_graph_inst.edge_degrees_batch(edge_pairs_tuples)
    )
    
    # ç¬¬3æ­¥: é‡æ„è¾¹æ•°æ®åˆ—è¡¨
    edge_datas = []
    for k in results:
        pair = (k["src_id"], k["tgt_id"])
        edge_props = edge_data_dict.get(pair)
        if edge_props is not None:
            combined = {
                "src_id": k["src_id"],
                "tgt_id": k["tgt_id"],
                "rank": edge_degrees_dict.get(pair, k.get("rank", 0)),
                "created_at": k.get("created_at", None),
                **edge_props
            }
            edge_datas.append(combined)
    
    # ç¬¬4æ­¥: è¾¹æ’åºå’ŒTokenæˆªæ–­
    tokenizer = text_chunks_db.global_config.get("tokenizer")
    edge_datas = sorted(
        edge_datas, 
        key=lambda x: (x["rank"], x["weight"]), 
        reverse=True
    )
    edge_datas = truncate_list_by_token_size(
        edge_datas,
        key=lambda x: x["description"] if x["description"] is not None else "",
        max_token_size=query_param.max_token_for_global_context,
        tokenizer=tokenizer,
    )
    
    # ç¬¬5æ­¥: ä»å…³ç³»åå‘æŸ¥æ‰¾ç›¸å…³å®ä½“å’Œæ–‡æœ¬
    use_entities, use_text_units = await asyncio.gather(
        _find_most_related_entities_from_relationships(
            edge_datas, query_param, knowledge_graph_inst
        ),
        _find_related_text_unit_from_relationships(
            edge_datas, query_param, text_chunks_db, knowledge_graph_inst
        ),
    )
    
    return use_entities, edge_datas, use_text_units
```

#### 2.3.3 å…³é”®æŠ€æœ¯ç‚¹è¯´æ˜

**å®ä½“åº¦æ•°è®¡ç®—**:
- å›¾ä¸­èŠ‚ç‚¹çš„è¿æ¥è¾¹æ•°é‡ï¼Œè¡¨ç¤ºå®ä½“é‡è¦æ€§
- ç”¨äºæ’åºå’Œç­›é€‰æœ€é‡è¦çš„å®ä½“

**å…³ç³»æƒé‡è®¡ç®—**:
- ç´¯ç§¯å¤šæ¬¡æå–çš„å…³ç³»æƒé‡
- ç»“åˆåº¦æ•°è¿›è¡ŒåŒé‡æ’åº

**Tokené¢„ç®—ç®¡ç†**:
- ä½¿ç”¨`truncate_list_by_token_size`ç²¾ç¡®æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
- ä¼˜å…ˆä¿ç•™é«˜æƒé‡/é«˜åº¦æ•°çš„å®ä½“å’Œå…³ç³»

**æ‰¹é‡ä¼˜åŒ–**:
- ä½¿ç”¨`get_nodes_batch`å’Œ`get_edges_batch`å‡å°‘æ•°æ®åº“æŸ¥è¯¢æ¬¡æ•°
- å¹¶å‘æ‰§è¡Œå¤šä¸ªç‹¬ç«‹æŸ¥è¯¢æå‡æ€§èƒ½

### 2.4 å“åº”ç”ŸæˆæŠ€æœ¯

**æœ€ç»ˆä¸Šä¸‹æ–‡æ ¼å¼**:
```json
{
  "entities": [
    {
      "id": 1,
      "entity": "å®ä½“å",
      "type": "å®ä½“ç±»å‹", 
      "description": "æè¿°",
      "rank": "åº¦æ•°æ’å",
      "created_at": "åˆ›å»ºæ—¶é—´",
      "file_path": "æ–‡ä»¶è·¯å¾„"
    }
  ],
  "relationships": [
    {
      "id": 1,
      "entity1": "æºå®ä½“",
      "entity2": "ç›®æ ‡å®ä½“",
      "description": "å…³ç³»æè¿°", 
      "keywords": "å…³é”®è¯",
      "weight": "æƒé‡",
      "rank": "åº¦æ•°æ’å",
      "created_at": "åˆ›å»ºæ—¶é—´",
      "file_path": "æ–‡ä»¶è·¯å¾„"
    }
  ],
  "text_units": [
    {
      "id": 1,
      "content": "æ–‡æœ¬å†…å®¹",
      "file_path": "æ–‡ä»¶è·¯å¾„"
    }
  ]
}
```

**RAGå“åº”Prompt** (`rag_response`):
```
---Role---
You are a helpful assistant responding to user query about Knowledge Graph and Document Chunks provided in JSON format below.

---Goal---
Generate a concise response based on Knowledge Base and follow Response Rules, considering both the conversation history and the current query. Summarize all information in the provided Knowledge Base, and incorporating general knowledge relevant to the Knowledge Base. Do not include information not provided by Knowledge Base.

When handling relationships with timestamps:
1. Each relationship has a "created_at" timestamp indicating when we acquired this knowledge
2. When encountering conflicting relationships, consider both the semantic content and the timestamp
3. Don't automatically prefer the most recently created relationships - use judgment based on the context
4. For time-specific queries, prioritize temporal information in the content before considering creation timestamps

---Conversation History---
{history}

---Knowledge Graph and Document Chunks---
{context_data}

---Response Rules---
- Target format and length: {response_type}
- Use markdown formatting with appropriate section headings
- Please respond in the same language as the user's question.
- Ensure the response maintains continuity with the conversation history.
- List up to 5 most important reference sources at the end under "References" section. Clearly indicating whether each source is from Knowledge Graph (KG) or Document Chunks (DC), and include the file path if available, in the following format: [KG/DC] file_path
- If you don't know the answer, just say so.
- Do not make anything up. Do not include information not provided by the Knowledge Base.
- Addtional user prompt: {user_prompt}

Response:
```

## ä¸‰ã€æŠ€æœ¯ä¼˜åŒ–ç­–ç•¥

### 3.1 å¹¶å‘æ§åˆ¶
- **LLMå¹¶å‘é™åˆ¶**: `llm_model_max_async = 4`
- **Embeddingå¹¶å‘é™åˆ¶**: `embedding_func_max_async = 16`
- **æ–‡æ¡£å¹¶è¡Œå¤„ç†**: `max_parallel_insert = 2`
- **ä¿¡å·é‡æ§åˆ¶**: ä½¿ç”¨asyncio.Semaphoreé™åˆ¶å¹¶å‘

### 3.2 ç¼“å­˜æœºåˆ¶
- **LLMå“åº”ç¼“å­˜**: ç¼“å­˜å®ä½“æŠ½å–ã€æ€»ç»“ç­‰LLMè°ƒç”¨ç»“æœ
- **Embeddingç¼“å­˜**: æ”¯æŒç›¸ä¼¼åº¦é˜ˆå€¼çš„å‘é‡ç¼“å­˜
- **æŸ¥è¯¢ç¼“å­˜**: ç¼“å­˜å…³é”®è¯æå–å’ŒæŸ¥è¯¢ç»“æœ

### 3.3 Tokenç®¡ç†
- **åˆ†å—Tokené™åˆ¶**: é»˜è®¤1200 tokens per chunk
- **LLMæœ€å¤§Token**: é»˜è®¤32768 tokens
- **ä¸Šä¸‹æ–‡Tokené™åˆ¶**: 
  - Localä¸Šä¸‹æ–‡: `max_token_for_local_context`
  - Globalä¸Šä¸‹æ–‡: `max_token_for_global_context`  
  - æ–‡æœ¬å•å…ƒ: `max_token_for_text_unit`

### 3.4 é”™è¯¯å¤„ç†å’ŒçŠ¶æ€ç®¡ç†
- **æ–‡æ¡£çŠ¶æ€è·Ÿè¸ª**: PENDING â†’ PROCESSING â†’ PROCESSED/FAILED
- **ç®¡é“çŠ¶æ€é”**: é˜²æ­¢å¤šè¿›ç¨‹å†²çªçš„å…¨å±€é”æœºåˆ¶
- **å¼‚å¸¸æ¢å¤**: æ”¯æŒå¤„ç†å¤±è´¥æ–‡æ¡£çš„é‡è¯•

## å››ã€å­˜å‚¨ç³»ç»Ÿæ¶æ„

### 4.1 å­˜å‚¨ç±»å‹
- **å‘é‡å­˜å‚¨**: NanoVectorDBStorage (é»˜è®¤) / æ”¯æŒMilvusã€Qdrantç­‰
- **å›¾å­˜å‚¨**: NetworkXStorage (é»˜è®¤) / æ”¯æŒNeo4jã€MongoDBç­‰  
- **é”®å€¼å­˜å‚¨**: JsonKVStorage (é»˜è®¤) / æ”¯æŒRedisç­‰
- **çŠ¶æ€å­˜å‚¨**: JsonDocStatusStorage (é»˜è®¤)

### 4.2 æ•°æ®ä¸€è‡´æ€§
- **æ‰¹é‡æ“ä½œ**: ä½¿ç”¨æ‰¹é‡æ¥å£æå‡æ€§èƒ½
- **äº‹åŠ¡æ€§**: ç¡®ä¿å®ä½“ã€å…³ç³»ã€å‘é‡æ•°æ®çš„ä¸€è‡´æ€§
- **ç´¢å¼•åŒæ­¥**: æ‰€æœ‰å­˜å‚¨å®Œæˆåç»Ÿä¸€è°ƒç”¨index_done_callback

## äº”ã€æ ¸å¿ƒæŠ€æœ¯æ€»ç»“

**LightRAGçš„æŠ€æœ¯æ ˆ**:
1. **æ–‡æœ¬å¤„ç†**: tiktoken tokenizer + æ»‘åŠ¨çª—å£åˆ†å—
2. **å®ä½“æŠ½å–**: LLM + ç»“æ„åŒ–prompt + è¡¥å……æŠ½å–æœºåˆ¶  
3. **çŸ¥è¯†èåˆ**: æè¿°åˆå¹¶ + LLMæ€»ç»“ + æƒé‡ç´¯åŠ 
4. **å‘é‡æ£€ç´¢**: åˆ†å±‚å‘é‡æœç´¢(å®ä½“/å…³ç³»/æ–‡æ¡£å—)
5. **å›¾éå†**: åŸºäºåº¦æ•°å’Œæƒé‡çš„å›¾ç®—æ³•
6. **ä¸Šä¸‹æ–‡ç»„è£…**: å¤šæ¨¡æ€æ•°æ®èåˆ + Tokené¢„ç®—ç®¡ç†
7. **ç”Ÿæˆå¢å¼º**: ç»“æ„åŒ–ä¸Šä¸‹æ–‡ + RAG prompt engineering

**å…³é”®åˆ›æ–°ç‚¹**:
- **åˆ†å±‚æ£€ç´¢**: Local/Global/Hybridä¸‰ç§æ¨¡å¼é€‚åº”ä¸åŒæŸ¥è¯¢ç±»å‹
- **å¢é‡æ„å»º**: å®ä½“å…³ç³»çš„å¢é‡åˆå¹¶å’ŒLLMæ€»ç»“
- **å¤šæ¨¡æ€èåˆ**: å›¾ç»“æ„+å‘é‡æ£€ç´¢+åŸå§‹æ–‡æœ¬çš„èåˆ
- **å¹¶å‘ä¼˜åŒ–**: ç»†ç²’åº¦çš„å¼‚æ­¥å¹¶å‘æ§åˆ¶

## å…­ã€å¹¶å‘æ¶æ„é—®é¢˜ä¸å·¥ä½œæµå¼•æ“é€‚é…åˆ†æ

### 6.1 LightRAGçš„æ ¸å¿ƒå¹¶å‘é—®é¢˜

é€šè¿‡æ·±å…¥åˆ†æLightRAGæºä»£ç ï¼Œå‘ç°äº†ä¸¥é‡çš„æ¶æ„ç¼ºé™·ï¼š

#### 6.1.1 å…¨å±€å•ä¾‹çŠ¶æ€ç®¡ç†
```python
# lightrag/kg/shared_storage.py - å…¨å±€å˜é‡å¯¼è‡´å¤šå®ä¾‹å†²çª
_is_multiprocess = None
_manager = None 
_shared_dicts: Optional[Dict[str, Any]] = None
_pipeline_status_lock: Optional[LockType] = None
_storage_lock: Optional[LockType] = None
_graph_db_lock: Optional[LockType] = None
```

**é—®é¢˜å½±å“**:
- åŒä¸€è¿›ç¨‹ä¸­æ— æ³•åˆ›å»ºå¤šä¸ªç‹¬ç«‹çš„LightRAGå®ä¾‹
- å¤šä¸ªcollectionä¹‹é—´ä¼šå…±äº«ç®¡é“çŠ¶æ€ï¼Œäº’ç›¸å¹²æ‰°
- å…¨å±€é”å¯¼è‡´ä¸¥é‡çš„ä¸²è¡ŒåŒ–ç“¶é¢ˆ

#### 6.1.2 ç®¡é“çŠ¶æ€å†²çª
```python
# æ‰€æœ‰å®ä¾‹å…±äº«åŒä¸€ä¸ªpipeline_statusï¼Œå¯¼è‡´çŠ¶æ€è¦†ç›–
pipeline_status = await get_namespace_data("pipeline_status") 
async with pipeline_status_lock:
    if not pipeline_status.get("busy", False):
        # åªæœ‰ä¸€ä¸ªå®ä¾‹èƒ½æ‰§è¡Œï¼Œå…¶ä»–è¢«é˜»å¡
```

#### 6.1.3 äº‹ä»¶å¾ªç¯ç®¡ç†æ··ä¹±
```python
def always_get_an_event_loop() -> asyncio.AbstractEventLoop:
    try:
        current_loop = asyncio.get_event_loop()
        if current_loop.is_closed():
            raise RuntimeError("Event loop is closed.")
        return current_loop
    except RuntimeError:
        logger.info("Creating a new event loop in main thread.")
        new_loop = asyncio.new_event_loop()
        asyncio.set_event_loop(new_loop)
        return new_loop
```

åœ¨Prefect/Celeryçš„å·¥ä½œè€…è¿›ç¨‹ä¸­ï¼Œè¿™ç§äº‹ä»¶å¾ªç¯ç®¡ç†ä¼šå¯¼è‡´å†²çªã€‚

### 6.2 ä¸Celeryçš„é€‚é…æ€§åˆ†æ

#### 6.2.1 Celeryçš„æ‰§è¡Œæ¨¡å‹
```python
# Celeryä»»åŠ¡æ‰§è¡Œæ¨¡å‹
@app.task
def process_document(document_content, collection_id):
    # åœ¨å·¥ä½œè€…è¿›ç¨‹ä¸­æ‰§è¡Œ
    # æ¯ä¸ªå·¥ä½œè€…è¿›ç¨‹æœ‰è‡ªå·±çš„Pythonè§£é‡Šå™¨
    rag = LightRAG(working_dir=f"./collection_{collection_id}")
    return rag.insert(document_content)
```

**é—®é¢˜åˆ†æ**:
- âœ… **è¿›ç¨‹éš”ç¦»**: Celeryçš„å¤šè¿›ç¨‹æ¨¡å‹å¤©ç„¶é¿å…äº†LightRAGçš„å…¨å±€çŠ¶æ€å†²çª
- âŒ **äº‹ä»¶å¾ªç¯å†²çª**: Celeryå·¥ä½œè€…å¯èƒ½ä¸LightRAGçš„äº‹ä»¶å¾ªç¯ç®¡ç†å†²çª
- âŒ **èµ„æºé‡å¤**: æ¯ä¸ªä»»åŠ¡éƒ½è¦é‡æ–°åˆå§‹åŒ–LightRAGå®ä¾‹ï¼Œæ•ˆç‡ä½ä¸‹
- âŒ **çŠ¶æ€ä¸€è‡´æ€§**: éš¾ä»¥åœ¨ä»»åŠ¡é—´å…±äº«çŸ¥è¯†å›¾è°±çŠ¶æ€

#### 6.2.2 æ¨èçš„Celeryé€‚é…æ–¹æ¡ˆ

**æ–¹æ¡ˆä¸€: è¿›ç¨‹çº§å•ä¾‹**
```python
# celery_tasks.py
from lightrag import LightRAG
import asyncio

# å·¥ä½œè€…è¿›ç¨‹å¯åŠ¨æ—¶åˆå§‹åŒ–
_rag_instances = {}

@app.task
def process_collection_document(document, collection_id):
    if collection_id not in _rag_instances:
        _rag_instances[collection_id] = LightRAG(
            working_dir=f"./collection_{collection_id}"
        )
    
    rag = _rag_instances[collection_id]
    # ä½¿ç”¨æ–°çš„äº‹ä»¶å¾ªç¯é¿å…å†²çª
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(rag.ainsert(document))
    finally:
        loop.close()
```

### 6.3 ä¸Prefectçš„é€‚é…æ€§åˆ†æ

#### 6.3.1 Prefectçš„æ‰§è¡Œæ¨¡å‹
```python
# Prefectæµç¨‹æ¨¡å‹
from prefect import flow, task
import asyncio

@task
async def extract_entities(chunk, collection_id):
    # åœ¨Prefectçš„ä»»åŠ¡è¿è¡Œå™¨ä¸­æ‰§è¡Œ
    # å¯èƒ½åœ¨åŒä¸€è¿›ç¨‹ä¸­è¿è¡Œå¤šä¸ªä»»åŠ¡
    pass

@flow  
def process_collection(documents, collection_id):
    tasks = []
    for doc in documents:
        chunks = chunk_document(doc)
        for chunk in chunks:
            tasks.append(extract_entities.submit(chunk, collection_id))
    return tasks
```

**é—®é¢˜åˆ†æ**:
- âŒ **å…¨å±€çŠ¶æ€å†²çª**: åŒä¸€è¿›ç¨‹ä¸­çš„å¤šä¸ªPrefectä»»åŠ¡ä¼šå…±äº«LightRAGå…¨å±€çŠ¶æ€
- âŒ **å¼‚æ­¥å¾ªç¯åµŒå¥—**: Prefectçš„å¼‚æ­¥æ‰§è¡Œä¸LightRAGçš„äº‹ä»¶å¾ªç¯å¯èƒ½å†²çª
- âœ… **çŠ¶æ€ä¼ é€’**: Prefectçš„æµç¨‹æ¨¡å‹ä¾¿äºåœ¨ä»»åŠ¡é—´ä¼ é€’çŠ¶æ€
- âœ… **é”™è¯¯å¤„ç†**: Prefectçš„é‡è¯•å’Œé”™è¯¯å¤„ç†æœºåˆ¶è¾ƒå®Œå–„

#### 6.3.2 æ¨èçš„Prefecté€‚é…æ–¹æ¡ˆ

**æ–¹æ¡ˆä¸€: è‡ªå®ç°æ ¸å¿ƒç»„ä»¶**
```python
from prefect import flow, task
import asyncio
from typing import List, Dict, Any

@task
async def chunk_documents(documents: List[str]) -> List[Dict[str, Any]]:
    """æ— çŠ¶æ€æ–‡æ¡£åˆ†å—"""
    import tiktoken
    tokenizer = tiktoken.get_encoding("cl100k_base")
    
    chunks = []
    for doc_id, doc in enumerate(documents):
        # å®ç°LightRAGçš„åˆ†å—é€»è¾‘ï¼Œä½†æ— å…¨å±€çŠ¶æ€
        doc_chunks = chunking_by_token_size(tokenizer, doc, max_token_size=1200)
        for chunk in doc_chunks:
            chunk['doc_id'] = doc_id
            chunks.append(chunk)
    return chunks

@task  
async def extract_entities_batch(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """æ‰¹é‡å®ä½“æŠ½å–"""
    # å®ç°LightRAGçš„å®ä½“æŠ½å–é€»è¾‘ï¼Œä½†æ— å…¨å±€çŠ¶æ€
    async def extract_single_chunk(chunk):
        # è°ƒç”¨LLMè¿›è¡Œå®ä½“æŠ½å–
        prompt = build_entity_extraction_prompt(chunk['content'])
        result = await llm_call(prompt)
        return parse_entities_and_relations(result)
    
    results = await asyncio.gather(*[extract_single_chunk(chunk) for chunk in chunks])
    return results

@task
async def merge_and_store_entities(
    entities_batch: List[Dict[str, Any]], 
    collection_id: str
) -> None:
    """åˆå¹¶å¹¶å­˜å‚¨å®ä½“åˆ°å¤–éƒ¨æ•°æ®åº“"""
    # ä½¿ç”¨PostgreSQL/Neo4jç­‰å¤–éƒ¨æ•°æ®åº“ï¼Œé¿å…LightRAGçš„çŠ¶æ€ç®¡ç†
    await store_to_postgresql(entities_batch, collection_id)
    await store_to_neo4j(entities_batch, collection_id)

@flow
def process_collection_flow(documents: List[str], collection_id: str):
    """å¤„ç†å•ä¸ªcollectionçš„å®Œæ•´æµç¨‹"""
    chunks = chunk_documents(documents)
    entities = extract_entities_batch(chunks)
    merge_and_store_entities(entities, collection_id)
```

### 6.4 æ¨èæ¶æ„ï¼šè‡ªå®ç° + Prefect

åŸºäºåˆ†æï¼Œæœ€é€‚åˆå¤§è§„æ¨¡SaaSåœºæ™¯çš„æ–¹æ¡ˆæ˜¯ï¼š

#### 6.4.1 æ ¸å¿ƒç»„ä»¶è‡ªå®ç°
```python
# core/chunker.py - æ— çŠ¶æ€åˆ†å—å™¨
class StatelessChunker:
    def __init__(self, tokenizer_name: str = "cl100k_base"):
        self.tokenizer = tiktoken.get_encoding(tokenizer_name)
    
    def chunk_document(self, content: str, max_tokens: int = 1200) -> List[Dict]:
        # å®ç°LightRAGçš„chunking_by_token_sizeé€»è¾‘
        pass

# core/entity_extractor.py - æ— çŠ¶æ€å®ä½“æŠ½å–å™¨  
class StatelessEntityExtractor:
    def __init__(self, llm_func, embedding_func):
        self.llm_func = llm_func
        self.embedding_func = embedding_func
    
    async def extract_entities(self, chunk: str) -> Dict[str, Any]:
        # å®ç°LightRAGçš„extract_entitiesé€»è¾‘ï¼Œä½†æ— å…¨å±€çŠ¶æ€
        pass

# core/knowledge_merger.py - æ— çŠ¶æ€çŸ¥è¯†åˆå¹¶å™¨
class StatelessKnowledgeMerger:
    async def merge_entities(self, entities_list: List[Dict]) -> List[Dict]:
        # å®ç°LightRAGçš„_merge_nodes_then_upserté€»è¾‘
        pass
```

#### 6.4.2 Prefectæµç¨‹ç¼–æ’
```python
from prefect import flow, task
from core.chunker import StatelessChunker
from core.entity_extractor import StatelessEntityExtractor
from core.knowledge_merger import StatelessKnowledgeMerger

@task(retries=3)
async def process_document_chunks(
    document: str, 
    collection_id: str,
    chunker: StatelessChunker,
    extractor: StatelessEntityExtractor
) -> List[Dict[str, Any]]:
    """å¤„ç†å•ä¸ªæ–‡æ¡£çš„æ‰€æœ‰chunks"""
    chunks = chunker.chunk_document(document)
    
    results = []
    for chunk in chunks:
        entities = await extractor.extract_entities(chunk['content'])
        entities['chunk_id'] = chunk['chunk_id']
        entities['collection_id'] = collection_id
        results.append(entities)
    
    return results

@flow
def process_collection_parallel(documents: List[str], collection_id: str):
    """å¹¶è¡Œå¤„ç†collectionä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
    
    # åˆå§‹åŒ–æ— çŠ¶æ€ç»„ä»¶
    chunker = StatelessChunker()
    extractor = StatelessEntityExtractor(llm_func, embedding_func)
    merger = StatelessKnowledgeMerger()
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡æ¡£
    doc_tasks = []
    for doc in documents:
        task = process_document_chunks.submit(doc, collection_id, chunker, extractor)
        doc_tasks.append(task)
    
    # ç­‰å¾…æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆ
    all_entities = []
    for task in doc_tasks:
        entities = task.result()
        all_entities.extend(entities)
    
    # åˆå¹¶å’Œå­˜å‚¨
    merged_entities = merger.merge_entities(all_entities)
    store_to_databases.submit(merged_entities, collection_id)

@flow 
def process_multiple_collections(collections_data: Dict[str, List[str]]):
    """å¤„ç†å¤šä¸ªcollections"""
    collection_flows = []
    
    for collection_id, documents in collections_data.items():
        flow_task = process_collection_parallel.submit(documents, collection_id)
        collection_flows.append(flow_task)
    
    # ç­‰å¾…æ‰€æœ‰collectionå¤„ç†å®Œæˆ
    for flow_task in collection_flows:
        flow_task.result()
```

#### 6.4.3 å­˜å‚¨æ¶æ„
```python
# storage/postgresql_store.py
class PostgreSQLEntityStore:
    async def store_entities(self, entities: List[Dict], collection_id: str):
        # å­˜å‚¨å®ä½“åˆ°PostgreSQLï¼Œæ”¯æŒå¹¶å‘å†™å…¥
        pass

# storage/neo4j_store.py  
class Neo4jGraphStore:
    async def store_relationships(self, relations: List[Dict], collection_id: str):
        # å­˜å‚¨å…³ç³»åˆ°Neo4jï¼Œæ”¯æŒæ‰¹é‡æ“ä½œ
        pass
```

### 6.5 ä¼˜åŠ¿æ€»ç»“

**è‡ªå®ç°æ–¹æ¡ˆçš„ä¼˜åŠ¿**:
1. **çœŸæ­£çš„å¤šå®ä¾‹å¹¶å‘**: æ— å…¨å±€çŠ¶æ€å†²çª
2. **Prefectå®Œç¾é€‚é…**: åˆ©ç”¨Prefectçš„å¼‚æ­¥ä»»åŠ¡è°ƒåº¦
3. **æ°´å¹³æ‰©å±•**: æ”¯æŒK8så¤šPodå¹¶è¡Œå¤„ç†
4. **å­˜å‚¨åˆ†ç¦»**: PostgreSQL + Neo4jçš„ä¸“ä¸šå­˜å‚¨æ–¹æ¡ˆ
5. **é”™è¯¯éš”ç¦»**: å•ä¸ªcollectionå¤±è´¥ä¸å½±å“å…¶ä»–
6. **èµ„æºä¼˜åŒ–**: ç²¾ç¡®æ§åˆ¶LLMè°ƒç”¨å¹¶å‘åº¦

**éƒ¨ç½²å»ºè®®**:
- ä½¿ç”¨Prefectä½œä¸ºå·¥ä½œæµå¼•æ“
- PostgreSQLå­˜å‚¨å®ä½“å’Œå…ƒæ•°æ®
- Neo4jå­˜å‚¨çŸ¥è¯†å›¾è°±
- Redisä½œä¸ºç¼“å­˜å±‚
- é€šè¿‡K8så®ç°å¼¹æ€§æ‰©å®¹

## ä¸ƒã€LightRAGæ ¸å¿ƒæŠ€æœ¯ç‚¹å®Œæ•´åˆ—ä¸¾

### 7.1 å†™å…¥(Indexing)æµç¨‹æ ¸å¿ƒæŠ€æœ¯

#### ğŸ“„ **æ–‡æ¡£å¤„ç†æŠ€æœ¯**
1. **æ–‡æ¡£åˆ†å—ç®—æ³•** (`chunking_by_token_size`)
   - tiktoken tokenizerç²¾ç¡®è®¡ç®—tokenæ•°é‡
   - æ»‘åŠ¨çª—å£åˆ†å—ï¼š1200 tokens/å—ï¼Œ100 tokensé‡å 
   - æ”¯æŒå­—ç¬¦é¢„åˆ†å‰² + tokenåˆ†å‰²çš„æ··åˆç­–ç•¥
   - ä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§çš„é‡å è®¾è®¡

2. **æ–‡æœ¬æ¸…ç†å’Œæ ‡å‡†åŒ–**
   - `clean_text()`: å»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œç¬¦æ ‡å‡†åŒ–
   - `normalize_extracted_info()`: å®ä½“åç§°æ ‡å‡†åŒ–
   - ä¸­è‹±æ–‡æ ‡ç‚¹ç¬¦å·ç»Ÿä¸€å¤„ç†

#### ğŸ§  **LLMå®ä½“å…³ç³»æŠ½å–æŠ€æœ¯**
3. **å®ä½“æŠ½å–æ ¸å¿ƒç®—æ³•** (`extract_entities`)
   - ç»“æ„åŒ–promptå·¥ç¨‹ï¼šentity_extractionæ¨¡æ¿
   - 5æ­¥éª¤æŠ½å–ï¼šå®ä½“è¯†åˆ«â†’å…³ç³»è¯†åˆ«â†’å…³é”®è¯æå–â†’æ ¼å¼åŒ–â†’å®Œæˆæ ‡è®°
   - é»˜è®¤å®ä½“ç±»å‹ï¼šorganization, person, geo, event, category
   - åˆ†éš”ç¬¦é…ç½®ï¼š`<|>`, `##`, `<|COMPLETE|>`

4. **å…³ç³»æŠ½å–æ ¸å¿ƒç®—æ³•** (`_handle_single_relationship_extraction`)
   - å…³ç³»æ ¼å¼éªŒè¯ï¼šå¿…é¡»åŒ…å«relationshipæ ‡è¯†ç¬¦
   - æºç›®æ ‡å®ä½“æ¸…ç†å’Œæ ‡å‡†åŒ–
   - è‡ªç¯æ£€æµ‹ï¼šæº=ç›®æ ‡åˆ™ä¸¢å¼ƒ
   - å…³ç³»æƒé‡æå–ï¼šé»˜è®¤1.0
   - å…³é”®è¯å¤„ç†ï¼šä¸­æ–‡é€—å·è½¬è‹±æ–‡

5. **è¡¥å……æŠ½å–æœºåˆ¶**
   - `entity_continue_extraction`: æç¤ºLLMè¡¥å……é—æ¼
   - `entity_if_loop_extraction`: YES/NOåˆ¤æ–­æ˜¯å¦è¿˜æœ‰é—æ¼
   - `entity_extract_max_gleaning = 1`: æœ€å¤§è¡¥å……æ¬¡æ•°

#### ğŸ”„ **çŸ¥è¯†èåˆæŠ€æœ¯**
6. **å®ä½“åˆå¹¶ç®—æ³•** (`_merge_nodes_then_upsert`)
   - å®ä½“ç±»å‹é€‰æ‹©ï¼šé¢‘æ¬¡æœ€é«˜çš„ç±»å‹
   - æè¿°åˆå¹¶ï¼š`GRAPH_FIELD_SEP`è¿æ¥
   - LLMæ€»ç»“è§¦å‘ï¼šæè¿°ç‰‡æ®µâ‰¥10æ—¶è‡ªåŠ¨æ€»ç»“
   - æ¥æºè¿½è¸ªï¼šè®°å½•æ‰€æœ‰chunk_idå’Œfile_path

7. **å…³ç³»åˆå¹¶ç®—æ³•** (`_merge_edges_then_upsert`)
   - æƒé‡ç´¯åŠ ï¼šæ‰€æœ‰ç›¸åŒå…³ç³»æƒé‡ç›¸åŠ 
   - å…³é”®è¯å»é‡åˆå¹¶ï¼šé€—å·åˆ†éš”
   - æè¿°åˆå¹¶ï¼šç±»ä¼¼å®ä½“æè¿°ç­–ç•¥
   - èŠ‚ç‚¹å­˜åœ¨æ€§æ£€æŸ¥ï¼šè‡ªåŠ¨åˆ›å»ºç¼ºå¤±èŠ‚ç‚¹

8. **LLMè‡ªåŠ¨æ€»ç»“** (`_handle_entity_relation_summary`)
   - æ€»ç»“promptï¼š`summarize_entity_descriptions`
   - çŸ›ç›¾è§£å†³ï¼šè¦æ±‚LLMè§£å†³æè¿°å†²çª
   - ç¬¬ä¸‰äººç§°æè¿°ï¼šåŒ…å«å®ä½“åç§°çš„å®Œæ•´ä¸Šä¸‹æ–‡

#### ğŸ’¾ **å­˜å‚¨æ¶æ„æŠ€æœ¯**
9. **å¤šæ¨¡æ€å­˜å‚¨è®¾è®¡**
   - **å‘é‡å­˜å‚¨**ï¼šå®ä½“ã€å…³ç³»ã€æ–‡æ¡£å—ä¸‰å¥—å‘é‡åº“
   - **å›¾å­˜å‚¨**ï¼šNetworkX/Neo4jå­˜å‚¨çŸ¥è¯†å›¾è°±
   - **é”®å€¼å­˜å‚¨**ï¼šJSON/Rediså­˜å‚¨æ–‡æ¡£å’Œç¼“å­˜
   - **çŠ¶æ€å­˜å‚¨**ï¼šæ–‡æ¡£å¤„ç†çŠ¶æ€è·Ÿè¸ª

10. **å‘é‡åŒ–ç­–ç•¥**
    - å®ä½“å‘é‡åŒ–ï¼š`entity_name + "\n" + description`
    - å…³ç³»å‘é‡åŒ–ï¼š`keywords + src_id + tgt_id + description`
    - æ–‡æ¡£å—å‘é‡åŒ–ï¼šåŸå§‹chunkå†…å®¹

### 7.2 æŸ¥è¯¢(Retrieval)æµç¨‹æ ¸å¿ƒæŠ€æœ¯

#### ğŸ” **å…³é”®è¯æå–æŠ€æœ¯**
11. **åŒå±‚å…³é”®è¯æå–** (`extract_keywords_only`)
    - JSONæ ¼å¼è¾“å‡ºï¼šhigh_level_keywords + low_level_keywords
    - ç¤ºä¾‹é©±åŠ¨å­¦ä¹ ï¼š3ä¸ªæ ‡å‡†ç¤ºä¾‹
    - å¯¹è¯å†å²é›†æˆï¼šget_conversation_turnså¤„ç†
    - ç¼“å­˜æœºåˆ¶ï¼šé¿å…é‡å¤LLMè°ƒç”¨
    - æ­£åˆ™è§£æï¼š`re.search(r"\{.*\}", result)`

12. **å…³é”®è¯åˆ†ç±»ç­–ç•¥**
    - **High-level**: æŠ½è±¡æ¦‚å¿µã€ä¸»é¢˜ã€é¢†åŸŸæœ¯è¯­
    - **Low-level**: å…·ä½“å®ä½“ã€äººåã€åœ°åã€äº§å“å

#### ğŸ¯ **æŸ¥è¯¢æ¨¡å¼æŠ€æœ¯**
13. **LocalæŸ¥è¯¢æ¨¡å¼** (`_get_node_data`)
    - åŸºäºlow-level keywordsçš„å®ä½“å‘é‡æœç´¢
    - å®ä½“åº¦æ•°è®¡ç®—ï¼š`node_degrees_batch`
    - ä¸€è·³é‚»å±…éå†ï¼šè·å–ç›¸å…³æ–‡æœ¬å—
    - ç›¸å…³è¾¹æŸ¥æ‰¾ï¼š`_find_most_related_edges_from_entities`

14. **GlobalæŸ¥è¯¢æ¨¡å¼** (`_get_edge_data`)
    - åŸºäºhigh-level keywordsçš„å…³ç³»å‘é‡æœç´¢
    - å…³ç³»åº¦æ•°è®¡ç®—ï¼š`edge_degrees_batch`
    - åå‘å®ä½“æŸ¥æ‰¾ï¼šä»å…³ç³»æ‰¾åˆ°ç›¸å…³å®ä½“
    - æƒé‡æ’åºï¼šç»“åˆåº¦æ•°å’Œæƒé‡åŒé‡æ’åº

15. **HybridæŸ¥è¯¢æ¨¡å¼**
    - å¹¶å‘æ‰§è¡ŒLocal + GlobalæŸ¥è¯¢
    - ç»“æœåˆå¹¶å»é‡ï¼š`process_combine_contexts`
    - ä¸Šä¸‹æ–‡èåˆï¼šentities + relationships + text_units

#### ğŸ“Š **ä¸Šä¸‹æ–‡æ„å»ºæŠ€æœ¯**
16. **æ‰¹é‡æ•°æ®è·å–ä¼˜åŒ–**
    - `get_nodes_batch`: æ‰¹é‡è·å–å®ä½“è¯¦æƒ…
    - `get_edges_batch`: æ‰¹é‡è·å–å…³ç³»è¯¦æƒ…
    - `asyncio.gather`: å¹¶å‘æ‰§è¡Œå¤šä¸ªæŸ¥è¯¢
    - å‡å°‘æ•°æ®åº“å¾€è¿”æ¬¡æ•°

17. **Tokené¢„ç®—ç®¡ç†**
    - `truncate_list_by_token_size`: ç²¾ç¡®æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
    - ä¼˜å…ˆçº§æ’åºï¼šé«˜åº¦æ•°/é«˜æƒé‡ä¼˜å…ˆ
    - åˆ†å±‚é™åˆ¶ï¼šlocal_context, global_context, text_unitåˆ†åˆ«é™åˆ¶

18. **æ–‡æœ¬å—å…³è”ç®—æ³•** (`_find_most_related_text_unit_from_entities`)
    - source_idè§£æï¼š`GRAPH_FIELD_SEP`åˆ†å‰²
    - ä¸€è·³é‚»å±…æ–‡æœ¬å—æŸ¥æ‰¾
    - relation_countsæ’åºï¼šå…³ç³»æ•°é‡ä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
    - æ‰¹é‡æ–‡æœ¬å—è·å–ï¼š5ä¸ªä¸€æ‰¹é¿å…èµ„æºè¿‡è½½

#### ğŸ¨ **å“åº”ç”ŸæˆæŠ€æœ¯**
19. **ç»“æ„åŒ–ä¸Šä¸‹æ–‡æ ¼å¼**
    - JSONæ ¼å¼ï¼šentities + relationships + text_units
    - å…ƒæ•°æ®åŒ…å«ï¼šid, rank, created_at, file_path
    - æ—¶é—´æˆ³å¤„ç†ï¼šUnixæ—¶é—´æˆ³è½¬å¯è¯»æ ¼å¼

20. **RAGå“åº”å·¥ç¨‹** (`rag_response`)
    - è§’è‰²å®šä¹‰ï¼šKnowledge GraphåŠ©æ‰‹
    - æ—¶é—´æˆ³å†²çªå¤„ç†ï¼šè¯­ä¹‰å†…å®¹ä¼˜å…ˆäºåˆ›å»ºæ—¶é—´
    - å¼•ç”¨æ ¼å¼ï¼š[KG/DC] file_pathæ ‡å‡†æ ¼å¼
    - å¯¹è¯å†å²é›†æˆï¼šä¿æŒè¿ç»­æ€§

### 7.3 ç³»ç»Ÿä¼˜åŒ–æŠ€æœ¯

#### âš¡ **å¹¶å‘æ§åˆ¶æŠ€æœ¯**
21. **å¤šå±‚æ¬¡å¹¶å‘é™åˆ¶**
    - LLMå¹¶å‘ï¼š`llm_model_max_async = 4`
    - Embeddingå¹¶å‘ï¼š`embedding_func_max_async = 16`
    - æ–‡æ¡£å¹¶è¡Œï¼š`max_parallel_insert = 2`
    - ä¿¡å·é‡æ§åˆ¶ï¼š`asyncio.Semaphore`

22. **å¼‚æ­¥ç¼–ç¨‹æ¨¡å¼**
    - `asyncio.gather`: å¹¶å‘æ‰§è¡Œç‹¬ç«‹ä»»åŠ¡
    - `asyncio.create_task`: ä»»åŠ¡åˆ›å»ºå’Œç®¡ç†
    - äº‹ä»¶å¾ªç¯ç®¡ç†ï¼š`always_get_an_event_loop`
    - å¼‚å¸¸å¤„ç†ï¼š`asyncio.wait(return_when=FIRST_EXCEPTION)`

#### ğŸ’¾ **ç¼“å­˜æœºåˆ¶æŠ€æœ¯**
23. **å¤šçº§ç¼“å­˜ç­–ç•¥**
    - LLMå“åº”ç¼“å­˜ï¼šå®ä½“æŠ½å–ã€æ€»ç»“ç»“æœ
    - Embeddingç¼“å­˜ï¼šç›¸ä¼¼åº¦é˜ˆå€¼ç¼“å­˜
    - æŸ¥è¯¢ç¼“å­˜ï¼šå…³é”®è¯æå–å’ŒæŸ¥è¯¢ç»“æœ
    - ç¼“å­˜é”®è®¡ç®—ï¼š`compute_args_hash`

24. **ç¼“å­˜æ•°æ®ç»“æ„**
    - `CacheData`: args_hash, content, prompt, quantizedç­‰
    - å‘é‡é‡åŒ–ï¼šå‡å°‘å­˜å‚¨ç©ºé—´
    - ç¼“å­˜ç±»å‹åŒºåˆ†ï¼šextract, query, keywords

#### ğŸ”§ **é”™è¯¯å¤„ç†æŠ€æœ¯**
25. **çŠ¶æ€ç®¡ç†ç³»ç»Ÿ**
    - æ–‡æ¡£çŠ¶æ€ï¼šPENDING â†’ PROCESSING â†’ PROCESSED/FAILED
    - ç®¡é“çŠ¶æ€é”ï¼š`pipeline_status_lock`é˜²æ­¢å†²çª
    - é”™è¯¯æ¢å¤ï¼šå¤±è´¥æ–‡æ¡£é‡è¯•æœºåˆ¶
    - çŠ¶æ€æŒä¹…åŒ–ï¼š`DocStatusStorage`

26. **å¼‚å¸¸éš”ç¦»æœºåˆ¶**
    - ä»»åŠ¡çº§å¼‚å¸¸å¤„ç†ï¼šå•ä¸ªæ–‡æ¡£å¤±è´¥ä¸å½±å“å…¶ä»–
    - èµ„æºæ¸…ç†ï¼šå¼‚å¸¸æ—¶çš„å­˜å‚¨å›æ»š
    - é‡è¯•ç­–ç•¥ï¼šå¯é…ç½®çš„é‡è¯•æ¬¡æ•°å’Œé—´éš”

### 7.4 æ¶æ„é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### âš ï¸ **å·²çŸ¥æ¶æ„ç¼ºé™·**
27. **å…¨å±€çŠ¶æ€å†²çª**
    - å…¨å±€å˜é‡ï¼š`_shared_dicts`, `_pipeline_status_lock`
    - å•ä¾‹æ¨¡å¼ï¼šåŒè¿›ç¨‹å¤šå®ä¾‹å†²çª
    - äº‹ä»¶å¾ªç¯ç®¡ç†ï¼šä¸å¤–éƒ¨æ¡†æ¶å†²çª

28. **å¹¶å‘ç“¶é¢ˆ**
    - ç®¡é“çŠ¶æ€å…±äº«ï¼šæ‰€æœ‰å®ä¾‹å…±äº«busyçŠ¶æ€
    - å…¨å±€é”ç«äº‰ï¼šä¸¥é‡ä¸²è¡ŒåŒ–ç“¶é¢ˆ
    - èµ„æºé‡å¤åˆå§‹åŒ–ï¼šæ¯ä¸ªå®ä¾‹é‡å¤åŠ è½½

#### âœ… **æ¨èè§£å†³æ–¹æ¡ˆ**
29. **æ— çŠ¶æ€ç»„ä»¶è®¾è®¡**
    - åˆ†å—å™¨ï¼šStatelessChunker
    - å®ä½“æŠ½å–å™¨ï¼šStatelessEntityExtractor  
    - çŸ¥è¯†åˆå¹¶å™¨ï¼šStatelessKnowledgeMerger
    - æŸ¥è¯¢å¤„ç†å™¨ï¼šStatelessQueryProcessor

30. **å·¥ä½œæµå¼•æ“é›†æˆ**
    - Prefectæµç¨‹ç¼–æ’ï¼š`@flow`, `@task`è£…é¥°å™¨
    - ä»»åŠ¡å¹¶å‘æ§åˆ¶ï¼šç²¾ç¡®çš„èµ„æºç®¡ç†
    - é”™è¯¯å¤„ç†ï¼šå†…ç½®é‡è¯•å’Œæ•…éšœæ¢å¤
    - çŠ¶æ€ä¼ é€’ï¼šä»»åŠ¡é—´æ•°æ®æµç®¡ç†

è¿™å°±æ˜¯LightRAGçš„**å…¨éƒ¨æ ¸å¿ƒæŠ€æœ¯ç‚¹**ï¼ä»æ–‡æ¡£åˆ†å—åˆ°æœ€ç»ˆå“åº”ç”Ÿæˆï¼Œæ¶µç›–äº†**30ä¸ªä¸»è¦æŠ€æœ¯ç‚¹**ï¼Œæ¯ä¸ªéƒ½æœ‰å…·ä½“çš„å®ç°ç»†èŠ‚å’Œä¼˜åŒ–ç­–ç•¥ã€‚ 